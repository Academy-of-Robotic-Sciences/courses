---
id: ai-205
title: "AI-205: Autonomous System Integration and Design"
sidebar_position: 5
version: 2.0.0
link: https://robotcampus.dev/styles/course-styles.css
---

# AI-205: Autonomous System Integration and Design

## Course Overview

| | |
|---|---|
| **Course Code** | AI-205 |
| **Duration** | 8 hours |
| **Level** | Capstone Project |
| **Prerequisites** | AI-201, AI-202, AI-203, AI-204 |
| **Format** | Academic lecture with intensive laboratory capstone project |
| **Theory/Practice** | 60% system architecture theory / 40% capstone implementation |

## Learning Objectives

### Theoretical Objectives
- Understand system architecture principles for autonomous robotic systems
- Analyze integration patterns for perception, planning, and control subsystems
- Examine state machine design and reactive planning frameworks
- Evaluate decision-making architectures for autonomous manipulation
- Study failure modes and robustness analysis in integrated systems

### Practical Objectives (Capstone Project)
- Design and implement a complete autonomous manipulation system
- Integrate computer vision, AI policies, and motion control
- Develop robust state machines for task execution
- Deploy and evaluate an end-to-end autonomous sorting system

---

## Module 1: System Architecture for Autonomous Robots

### 1.1 Historical Context of Autonomous Systems

<div class="historical-note">

The quest for autonomous machines dates to ancient automatons, but modern autonomous robotics began with Shakey the Robot at Stanford Research Institute (1966-1972). Shakey integrated vision, natural language processing, and planning—demonstrating that perception, cognition, and action could be unified in a single system.

The subsumption architecture introduced by Brooks (1986) challenged the sense-plan-act paradigm, proposing that intelligence emerges from layered reactive behaviors rather than explicit symbolic reasoning. This debate between deliberative and reactive planning shaped decades of research.

The modern era of autonomous systems integrates both approaches: reactive behaviors for low-level control, deliberative planning for high-level goals, and machine learning for perception and adaptation. Contemporary autonomous vehicles, warehouse robots, and manipulation systems exemplify this hybrid architecture.

</div>

Autonomous robotics requires the integration of multiple subsystems, each addressing a fundamental question:
- **Perception:** Where am I? What is around me?
- **Planning:** What should I do to achieve my goal?
- **Control:** How do I execute my plan?
- **Monitoring:** Is my plan succeeding? What should I do if it fails?

### 1.2 The Sense-Plan-Act Paradigm

The classical architecture for autonomous systems follows a sequential pipeline:

<div class="theory-concept">

**Sense-Plan-Act Cycle:**

1. **Sense:** Acquire sensor data $o_t$ from the environment
2. **Perceive:** Process observations to estimate state $s_t = h(o_t)$
3. **Plan:** Compute action sequence $\{a_t, a_{t+1}, ...\}$ that achieves goal $g$
4. **Act:** Execute next action $a_t$
5. **Repeat**

This cycle operates at a characteristic frequency (e.g., 10-50 Hz for manipulation tasks).

</div>

**Advantages:**
- Modularity: Each component can be developed and tested independently
- Interpretability: Clear separation of responsibilities
- Reusability: Components transfer across tasks

**Limitations:**
- Latency: Sequential processing introduces delay
- Brittleness: Failures in one module cascade through the system
- Inefficiency: Repeated re-planning for dynamic environments

### 1.3 Hierarchical Control Architecture

Complex tasks naturally decompose into hierarchical structures. A three-layer architecture is common:

<div class="theory-concept">

**Three-Layer Architecture:**

1. **Deliberative Layer (High Level):**
   - Task planning and scheduling
   - Operates at slow timescale (seconds to minutes)
   - Generates abstract plans: sequence of subtasks
   - Example: "Pick red block, then blue block, then place each in corresponding bin"

2. **Executive Layer (Middle Level):**
   - Reactive sequencing and coordination
   - Operates at medium timescale (0.1-1 seconds)
   - Monitors plan execution, handles local failures
   - Example: State machine coordinating pick-place primitives

3. **Behavioral Layer (Low Level):**
   - Reactive motor control
   - Operates at fast timescale (1-100 Hz)
   - Ensures smooth, safe motion
   - Example: Impedance control, collision avoidance

</div>

**Information Flow:**
- **Downward:** High-level plans constrain lower levels
- **Upward:** Low-level feedback informs higher levels (e.g., failure signals)

### 1.4 State Machines for Task Execution

Finite State Machines (FSMs) provide a formal framework for encoding task logic:

<div class="theory-concept">

**Definition 1.1 (Finite State Machine):** An FSM is defined by the tuple $(\mathcal{Q}, q_0, \Sigma, \delta, \mathcal{F})$ where:

- $\mathcal{Q}$ is a finite set of states
- $q_0 \in \mathcal{Q}$ is the initial state
- $\Sigma$ is the input alphabet (events/conditions)
- $\delta: \mathcal{Q} \times \Sigma \rightarrow \mathcal{Q}$ is the transition function
- $\mathcal{F} \subseteq \mathcal{Q}$ is the set of accepting (goal) states

Each state $q \in \mathcal{Q}$ corresponds to a behavior (e.g., "searching for object," "grasping," "transporting").

</div>

**Example: Block Sorting State Machine**

```python
from enum import Enum
from typing import Optional

class State(Enum):
    """States for autonomous block sorting task."""
    IDLE = 0
    DETECT_OBJECTS = 1
    SELECT_TARGET = 2
    APPROACH_OBJECT = 3
    GRASP_OBJECT = 4
    LIFT_OBJECT = 5
    DETECT_CONTAINER = 6
    MOVE_TO_CONTAINER = 7
    PLACE_OBJECT = 8
    RETRACT = 9
    ERROR = 10
    COMPLETE = 11


class Event(Enum):
    """Events triggering state transitions."""
    START = "start"
    OBJECTS_DETECTED = "objects_detected"
    NO_OBJECTS = "no_objects"
    TARGET_SELECTED = "target_selected"
    APPROACH_COMPLETE = "approach_complete"
    GRASP_SUCCESS = "grasp_success"
    GRASP_FAILURE = "grasp_failure"
    LIFT_COMPLETE = "lift_complete"
    CONTAINER_DETECTED = "container_detected"
    MOVE_COMPLETE = "move_complete"
    PLACE_SUCCESS = "place_success"
    PLACE_FAILURE = "place_failure"
    RETRACT_COMPLETE = "retract_complete"
    ERROR_OCCURRED = "error_occurred"
    RESET = "reset"


class TaskStateMachine:
    """
    State machine for autonomous manipulation task.

    Implements formal FSM with transition logic and action execution.
    """
    def __init__(self):
        self.state = State.IDLE
        self.target_object = None
        self.target_container = None

        # Transition table: (current_state, event) -> next_state
        self.transitions = {
            (State.IDLE, Event.START): State.DETECT_OBJECTS,

            (State.DETECT_OBJECTS, Event.OBJECTS_DETECTED): State.SELECT_TARGET,
            (State.DETECT_OBJECTS, Event.NO_OBJECTS): State.COMPLETE,

            (State.SELECT_TARGET, Event.TARGET_SELECTED): State.APPROACH_OBJECT,

            (State.APPROACH_OBJECT, Event.APPROACH_COMPLETE): State.GRASP_OBJECT,
            (State.APPROACH_OBJECT, Event.ERROR_OCCURRED): State.ERROR,

            (State.GRASP_OBJECT, Event.GRASP_SUCCESS): State.LIFT_OBJECT,
            (State.GRASP_OBJECT, Event.GRASP_FAILURE): State.RETRACT,

            (State.LIFT_OBJECT, Event.LIFT_COMPLETE): State.DETECT_CONTAINER,

            (State.DETECT_CONTAINER, Event.CONTAINER_DETECTED): State.MOVE_TO_CONTAINER,
            (State.DETECT_CONTAINER, Event.ERROR_OCCURRED): State.ERROR,

            (State.MOVE_TO_CONTAINER, Event.MOVE_COMPLETE): State.PLACE_OBJECT,

            (State.PLACE_OBJECT, Event.PLACE_SUCCESS): State.RETRACT,
            (State.PLACE_OBJECT, Event.PLACE_FAILURE): State.ERROR,

            (State.RETRACT, Event.RETRACT_COMPLETE): State.DETECT_OBJECTS,

            (State.ERROR, Event.RESET): State.IDLE,
        }

    def transition(self, event: Event) -> bool:
        """
        Attempt state transition based on event.

        Args:
            event: Event triggering transition

        Returns:
            success: True if transition was valid
        """
        key = (self.state, event)
        if key in self.transitions:
            new_state = self.transitions[key]
            print(f"Transition: {self.state.name} --[{event.value}]--> {new_state.name}")
            self.state = new_state
            return True
        else:
            print(f"Invalid transition: {self.state.name} with event {event.value}")
            return False

    def execute_state_action(self, perception_data, robot_interface):
        """
        Execute action associated with current state.

        This method implements the behavior for each state,
        interfacing with perception and control systems.
        """
        if self.state == State.IDLE:
            return None

        elif self.state == State.DETECT_OBJECTS:
            # Call vision system to detect objects
            objects = perception_data.get('detected_objects', [])
            if len(objects) > 0:
                return Event.OBJECTS_DETECTED
            else:
                return Event.NO_OBJECTS

        elif self.state == State.SELECT_TARGET:
            # Select object with highest priority
            objects = perception_data['detected_objects']
            self.target_object = self.select_object_by_priority(objects)
            return Event.TARGET_SELECTED

        elif self.state == State.APPROACH_OBJECT:
            # Execute motion to pre-grasp pose
            success = robot_interface.move_to_pregrasp(self.target_object)
            if success:
                return Event.APPROACH_COMPLETE
            else:
                return Event.ERROR_OCCURRED

        # ... (additional state behaviors)

    def select_object_by_priority(self, objects):
        """
        Select next object based on priority (e.g., color, position).

        This implements the task-specific logic.
        """
        # Example: prioritize red objects
        red_objects = [obj for obj in objects if obj['color'] == 'red']
        if red_objects:
            return red_objects[0]
        return objects[0]
```

### 1.5 Behavior Trees as Alternative

An alternative to FSMs for complex task logic is Behavior Trees (BTs):

<div class="alternative-approach">

**Behavior Trees:** Hierarchical structures representing task decomposition:

- **Nodes:**
  - **Sequence:** Execute children in order; fail if any fails
  - **Selector (Fallback):** Try children in order; succeed if any succeeds
  - **Parallel:** Execute children concurrently
  - **Decorator:** Modify child behavior (e.g., repeat, invert)

- **Advantages over FSMs:**
  - More modular and composable
  - Easier to design and maintain for complex tasks
  - Natural representation of priorities and fallbacks

- **Example Structure:**
```
Selector (Main Task)
├── Sequence (Sort Red Blocks)
│   ├── Condition: Red block detected?
│   ├── Action: Pick red block
│   └── Action: Place in red container
└── Sequence (Sort Blue Blocks)
    ├── Condition: Blue block detected?
    ├── Action: Pick blue block
    └── Action: Place in blue container
```

</div>

### Module 1 Quiz

1. **Architecture Comparison:** Compare the sense-plan-act paradigm with subsumption architecture. What are the fundamental philosophical differences?

2. **Hierarchical Control:** Why is a three-layer architecture appropriate for manipulation tasks? What happens if the deliberative layer is too slow?

3. **State Machine Design:** Design a state machine for a robot that must navigate to a table, pick up a cup, and deliver it to a person. Include error states and recovery transitions.

4. **Behavior Trees:** When would a behavior tree be preferable to a finite state machine? Give a concrete example where BT modularity provides significant advantage.

---

## Module 2: Perception-Action Integration

### 2.1 The Perception-Action Loop

Autonomous systems must close the loop from perception to action. The quality of this integration determines task success.

<div class="theory-concept">

**Perception-Action Coupling:**

For manipulation tasks, the key coupling is:
$$a_t = \pi(o_t, g)$$

where:
- $o_t$ is the current observation (e.g., camera image)
- $g$ is the task goal specification
- $a_t$ is the motor command

This can be:
1. **Reactive:** Immediate mapping (e.g., visual servoing, learned policy)
2. **Deliberative:** Intermediate planning (e.g., grasp detection → motion planning → execution)

</div>

### 2.2 Vision-Based Manipulation

Computer vision provides state estimates for manipulation:

**Object Detection:** Identify and localize objects of interest
$$\{(c_i, \mathbf{x}_i, \theta_i)\}_{i=1}^N$$
where $c_i$ is object class, $\mathbf{x}_i \in \mathbb{R}^3$ is 3D position, $\theta_i$ is orientation.

**Pose Estimation:** Compute 6-DOF pose (position + orientation) for grasping
$$T_{\text{obj}}^{\text{cam}} = \begin{bmatrix} R & \mathbf{t} \\ 0 & 1 \end{bmatrix} \in SE(3)$$

**Example: Object Detection Integration**

```python
import numpy as np
from dataclasses import dataclass
from typing import List

@dataclass
class DetectedObject:
    """Represents a detected object with pose."""
    label: str
    position: np.ndarray  # (x, y, z) in camera frame
    orientation: np.ndarray  # Quaternion (w, x, y, z)
    confidence: float
    bounding_box: np.ndarray  # (x_min, y_min, x_max, y_max) in image

class PerceptionModule:
    """
    Vision-based object detection and pose estimation.

    Integrates deep learning detection with geometric pose estimation.
    """
    def __init__(self, camera_intrinsics, detector_model):
        self.K = camera_intrinsics  # 3x3 camera matrix
        self.detector = detector_model
        self.depth_scale = 0.001  # Depth units to meters

    def process_frame(self, rgb_image, depth_image) -> List[DetectedObject]:
        """
        Detect objects and estimate their 3D poses.

        Args:
            rgb_image: RGB image (H, W, 3)
            depth_image: Depth image (H, W) in mm

        Returns:
            objects: List of detected objects with poses
        """
        # 1. Run 2D object detection
        detections_2d = self.detector.detect(rgb_image)

        # 2. Estimate 3D pose for each detection
        objects = []
        for det in detections_2d:
            # Extract bounding box center
            bbox = det['bounding_box']
            center_u = int((bbox[0] + bbox[2]) / 2)
            center_v = int((bbox[1] + bbox[3]) / 2)

            # Get depth at object center
            depth = depth_image[center_v, center_u] * self.depth_scale

            # Back-project to 3D
            position_cam = self.pixel_to_3d(center_u, center_v, depth)

            # Estimate orientation (simplified: assume upright)
            orientation = np.array([1, 0, 0, 0])  # Identity quaternion

            obj = DetectedObject(
                label=det['class_name'],
                position=position_cam,
                orientation=orientation,
                confidence=det['confidence'],
                bounding_box=bbox
            )
            objects.append(obj)

        return objects

    def pixel_to_3d(self, u, v, depth):
        """
        Back-project pixel to 3D point in camera frame.

        Using pinhole camera model:
        [u]   [fx  0  cx] [X/Z]
        [v] = [0  fy  cy] [Y/Z]
        [1]   [0   0   1] [1  ]
        """
        fx, fy = self.K[0, 0], self.K[1, 1]
        cx, cy = self.K[0, 2], self.K[1, 2]

        x = (u - cx) * depth / fx
        y = (v - cy) * depth / fy
        z = depth

        return np.array([x, y, z])

    def filter_by_workspace(self, objects, workspace_bounds):
        """
        Filter objects outside robot workspace.

        Args:
            objects: List of DetectedObject
            workspace_bounds: Dict with 'x', 'y', 'z' ranges

        Returns:
            filtered: Objects within workspace
        """
        filtered = []
        for obj in objects:
            x, y, z = obj.position
            if (workspace_bounds['x'][0] <= x <= workspace_bounds['x'][1] and
                workspace_bounds['y'][0] <= y <= workspace_bounds['y'][1] and
                workspace_bounds['z'][0] <= z <= workspace_bounds['z'][1]):
                filtered.append(obj)
        return filtered
```

### 2.3 Coordinate Frame Transformations

Manipulation requires transforming between multiple coordinate frames:

<div class="theory-concept">

**Frame Hierarchy:**
- **World Frame:** Global reference
- **Camera Frame:** Optical sensor perspective
- **Robot Base Frame:** Robot's fixed base
- **End-Effector Frame:** Gripper/tool
- **Object Frame:** Object being manipulated

**Transformation Chain:**
To grasp an object detected in camera frame, we need:
$$T_{\text{base}}^{\text{obj}} = T_{\text{base}}^{\text{cam}} \cdot T_{\text{cam}}^{\text{obj}}$$

where each transformation is a 4×4 homogeneous matrix in $SE(3)$.

</div>

**Example: Coordinate Transformation**

```python
class TransformManager:
    """
    Manages coordinate frame transformations for manipulation.

    Maintains transform tree and provides lookup/composition.
    """
    def __init__(self):
        # Store transformations as 4x4 matrices
        self.transforms = {}

    def set_transform(self, parent_frame, child_frame, transform):
        """
        Set transformation from parent to child frame.

        Args:
            parent_frame: Parent frame name
            child_frame: Child frame name
            transform: 4x4 homogeneous transformation matrix
        """
        key = (parent_frame, child_frame)
        self.transforms[key] = transform

        # Also store inverse
        inv_key = (child_frame, parent_frame)
        self.transforms[inv_key] = np.linalg.inv(transform)

    def lookup_transform(self, from_frame, to_frame):
        """
        Look up transformation from one frame to another.

        Args:
            from_frame: Source frame
            to_frame: Target frame

        Returns:
            transform: 4x4 matrix transforming points from from_frame to to_frame
        """
        # Direct lookup
        key = (from_frame, to_frame)
        if key in self.transforms:
            return self.transforms[key]

        # Try to find chain (simplified: assumes single intermediate)
        for intermediate in self.get_all_frames():
            key1 = (from_frame, intermediate)
            key2 = (intermediate, to_frame)
            if key1 in self.transforms and key2 in self.transforms:
                # Compose transformations
                return self.transforms[key2] @ self.transforms[key1]

        raise ValueError(f"No transform path from {from_frame} to {to_frame}")

    def transform_point(self, point, from_frame, to_frame):
        """
        Transform a 3D point between frames.

        Args:
            point: 3D point as (x, y, z) array
            from_frame: Source frame
            to_frame: Target frame

        Returns:
            transformed_point: 3D point in target frame
        """
        T = self.lookup_transform(from_frame, to_frame)

        # Convert to homogeneous coordinates
        point_hom = np.append(point, 1)

        # Apply transformation
        transformed_hom = T @ point_hom

        # Convert back to 3D
        return transformed_hom[:3]

    def get_all_frames(self):
        """Return set of all frame names."""
        frames = set()
        for (parent, child) in self.transforms.keys():
            frames.add(parent)
            frames.add(child)
        return frames

def create_transform(position, orientation):
    """
    Create 4x4 homogeneous transformation matrix.

    Args:
        position: (x, y, z) translation
        orientation: (w, x, y, z) quaternion

    Returns:
        T: 4x4 transformation matrix
    """
    # Convert quaternion to rotation matrix
    R = quaternion_to_rotation_matrix(orientation)

    # Construct homogeneous matrix
    T = np.eye(4)
    T[:3, :3] = R
    T[:3, 3] = position

    return T

def quaternion_to_rotation_matrix(q):
    """Convert quaternion (w, x, y, z) to 3x3 rotation matrix."""
    w, x, y, z = q

    R = np.array([
        [1 - 2*(y**2 + z**2),     2*(x*y - w*z),     2*(x*z + w*y)],
        [    2*(x*y + w*z), 1 - 2*(x**2 + z**2),     2*(y*z - w*x)],
        [    2*(x*z - w*y),     2*(y*z + w*x), 1 - 2*(x**2 + y**2)]
    ])

    return R
```

### 2.4 Action Selection and Execution

Given perceived state, the system must select and execute actions:

**Grasp Selection:** Choose grasp pose from object pose
$$T_{\text{grasp}} = T_{\text{obj}} \cdot T_{\text{obj}}^{\text{grasp}}$$

where $T_{\text{obj}}^{\text{grasp}}$ is a pre-defined grasp offset (e.g., approach from above).

**Motion Planning:** Compute collision-free trajectory from current pose to grasp pose. This can use:
- Analytical inverse kinematics (fast, limited applicability)
- Sampling-based planners (RRT, PRM)
- Optimization-based planners (CHOMP, TrajOpt)
- Learned policies (imitation learning from demonstrations)

**Execution:** Send trajectory to low-level controller and monitor progress.

### 2.5 Uncertainty and Robustness

Real-world perception is noisy. Robustness requires handling uncertainty:

<div class="theory-concept">

**Perception Uncertainty:**
- **Detection False Positives:** Detecting objects that don't exist
- **Detection False Negatives:** Missing objects that do exist
- **Pose Error:** Inaccurate position/orientation estimates

**Mitigation Strategies:**
1. **Confidence Thresholding:** Reject detections below threshold
2. **Temporal Filtering:** Track objects across frames, reject transients
3. **Multi-view Integration:** Fuse information from multiple cameras
4. **Active Perception:** Move camera to get better view
5. **Compliant Grasping:** Use force control to handle pose errors

</div>

### Module 2 Quiz

1. **Coordinate Frames:** Explain why multiple coordinate frames are necessary for manipulation. What could go wrong if transformations are computed incorrectly?

2. **Perception Errors:** How do perception errors (false positives, pose inaccuracy) propagate through to action failures? Design a mitigation strategy.

3. **Reactive vs. Deliberative:** Compare reactive visual servoing with deliberative grasp planning. When is each approach preferable?

4. **Workspace Filtering:** Why is it important to filter detected objects by workspace boundaries before attempting manipulation? What could happen without this check?

---

## Module 3: Decision-Making and Planning

### 3.1 Task and Motion Planning (TAMP)

Complex manipulation requires coordinating discrete task decisions with continuous motion planning:

<div class="theory-concept">

**Task Planning:** Discrete symbolic planning over high-level actions
- State space: Discrete symbolic states (e.g., "block A on table," "gripper empty")
- Actions: Pick, place, move
- Goal: Achieve desired symbolic state

**Motion Planning:** Continuous trajectory planning
- State space: Robot configuration space $\mathcal{C} \subset \mathbb{R}^n$
- Actions: Continuous control inputs
- Constraints: Collision avoidance, kinematic limits

**TAMP Integration:** Task planner proposes action sequence; motion planner checks feasibility. If infeasible, task planner revises plan.

</div>

For simple tasks, a hand-coded state machine suffices. For complex tasks with many objects and constraints, automated planning is beneficial.

### 3.2 Reactive vs. Deliberative Control

The spectrum from reactive to deliberative control:

**Purely Reactive:**
- No internal model or prediction
- Direct sensor-motor mapping
- Example: Braitenberg vehicles, subsumption behaviors
- Fast, but limited to simple tasks

**Hybrid:**
- High-level deliberation proposes goals
- Low-level reactive execution
- Example: State machine coordinating learned policies
- Balances flexibility and responsiveness

**Purely Deliberative:**
- Full world model and lookahead search
- Optimal plans for complex tasks
- Example: Chess-playing robots
- Slow, can be brittle to model errors

**For manipulation:** Hybrid approach is typical—state machine provides task structure, learned or  classical controllers provide reactive execution.

### 3.3 Failure Detection and Recovery

Robustness requires detecting and recovering from failures:

<div class="theory-concept">

**Failure Modes in Manipulation:**
1. **Perception Failure:** Object not detected or mislocalized
2. **Grasp Failure:** Gripper fails to secure object
3. **Motion Failure:** Collision or kinematic infeasibility
4. **External Disturbance:** Object moved by external agent

**Detection Methods:**
- **Expected State Checking:** Compare actual state to expected
- **Force/Torque Monitoring:** Detect unexpected contacts
- **Vision Verification:** Check object in gripper after grasp

**Recovery Strategies:**
- **Retry:** Attempt same action again (with jitter)
- **Replan:** Compute new plan from current state
- **Abort:** Declare task failure, return to safe state

</div>

**Example: Failure Monitoring**

```python
class FailureMonitor:
    """
    Monitors task execution and detects failures.

    Implements multiple failure detection strategies.
    """
    def __init__(self, timeout_thresholds):
        self.timeouts = timeout_thresholds
        self.state_start_time = {}

    def check_grasp_success(self, gripper_force, threshold=0.5):
        """
        Verify successful grasp using force sensor.

        Args:
            gripper_force: Current gripper force (N)
            threshold: Minimum force indicating object held

        Returns:
            success: True if grasp detected
        """
        return gripper_force > threshold

    def check_state_timeout(self, state, current_time):
        """
        Check if state has exceeded timeout.

        Args:
            state: Current state name
            current_time: Current time (seconds)

        Returns:
            timeout: True if state exceeded allotted time
        """
        if state not in self.state_start_time:
            self.state_start_time[state] = current_time
            return False

        elapsed = current_time - self.state_start_time[state]
        if state in self.timeouts:
            return elapsed > self.timeouts[state]

        return False

    def check_object_in_gripper(self, camera_image, expected_object):
        """
        Verify object is in gripper using vision.

        Args:
            camera_image: Image from wrist camera
            expected_object: Object ID that should be grasped

        Returns:
            verified: True if object detected in gripper
        """
        # Run detection on wrist camera image
        detected = self.detect_in_gripper(camera_image)

        if detected is None:
            return False

        # Check if detected object matches expected
        return detected['id'] == expected_object

    def reset_state_timer(self, state):
        """Reset timer for a state (call on state entry)."""
        if state in self.state_start_time:
            del self.state_start_time[state]
```

### 3.4 Multi-Object Reasoning

Real tasks involve multiple objects with dependencies:

**Example: Sorting Task**
- Two blocks: red and blue
- Two containers: red bin and blue bin
- Goal: Each block in matching container
- Constraint: Can only hold one object at a time

**Planning Considerations:**
- **Object Selection:** Which object to pick first?
- **Reachability:** Is object currently reachable, or blocked?
- **Optimal Ordering:** Minimize total time/distance

For simple cases (few objects), exhaustive enumeration works. For complex cases, heuristic search or learned policies are needed.

### 3.5 Human-Robot Interaction Considerations

<div class="alternative-approach">

**Predictable Behavior:** Humans working near robots need to predict robot intent. Design principles:
- **Legible Motion:** Trajectories that clearly indicate goal
- **Hesitation:** Slow down near humans to signal caution
- **Signaling:** Use visual/audio cues for state changes

**Failure Communication:** When robot encounters failure, communicate to human:
- Visual indicators (lights, displays)
- Verbal explanations ("Unable to locate red block")
- Request for assistance ("Please remove obstruction")

This is particularly important for collaborative robots (cobots) in shared workspaces.

</div>

### Module 3 Quiz

1. **TAMP:** Explain the difference between task planning and motion planning. Why can't a single planner address both?

2. **Failure Recovery:** Design a failure recovery strategy for a grasp failure scenario. Consider: retry count limits, alternative grasps, and ultimate abort conditions.

3. **Multi-Object Planning:** For a sorting task with 5 objects of different colors, propose a strategy for selecting the next object to pick. What factors should influence this decision?

4. **Reactive vs. Deliberative:** A robot must navigate through a cluttered environment while avoiding moving obstacles. Should it use reactive or deliberative planning? Justify your answer.

---

## Module 4: System Integration Patterns

### 4.1 Software Architecture Patterns

Robust autonomous systems require well-structured software:

**Layered Architecture:**
```
┌─────────────────────────────────┐
│   Application Layer             │  High-level task logic
│   (State machine, planners)     │
├─────────────────────────────────┤
│   Abstraction Layer             │  Hardware-agnostic interfaces
│   (Perception API, Control API) │
├─────────────────────────────────┤
│   Driver Layer                  │  Hardware-specific code
│   (Camera drivers, robot API)   │
└─────────────────────────────────┘
```

**Benefits:**
- Modularity: Replace components without affecting others
- Testability: Mock lower layers for unit testing
- Portability: Port to new hardware by changing driver layer

### 4.2 Communication Patterns

Components must exchange data:

<div class="theory-concept">

**Publisher-Subscriber (Topics):**
- One-to-many asynchronous communication
- Publishers emit data; subscribers receive
- Example: Camera publishes images, multiple nodes subscribe
- Best for: Continuous data streams (sensor data)

**Request-Reply (Services):**
- One-to-one synchronous communication
- Client sends request, server sends reply
- Example: Request grasp plan, receive trajectory
- Best for: Occasional requests with expected responses

**Shared State (Parameters):**
- Global configuration accessible to all components
- Example: Workspace boundaries, gripper dimensions
- Best for: Static configuration

</div>

**Example: ROS2 Integration Pattern**

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import PoseStamped
from custom_msgs.srv import ComputeGrasp

class ManipulationController(Node):
    """
    Main controller node integrating perception and motion planning.

    Uses ROS2 topics and services for inter-process communication.
    """
    def __init__(self):
        super().__init__('manipulation_controller')

        # Subscribers for sensor data
        self.create_subscription(
            Image,
            '/camera/rgb/image_raw',
            self.image_callback,
            10
        )

        # Publishers for commands
        self.grasp_pub = self.create_publisher(
            PoseStamped,
            '/target_grasp_pose',
            10
        )

        # Service clients for computation
        self.grasp_client = self.create_client(
            ComputeGrasp,
            'compute_grasp'
        )

        # Internal state
        self.latest_image = None
        self.state_machine = TaskStateMachine()

    def image_callback(self, msg):
        """Receive and store latest camera image."""
        self.latest_image = msg

    def request_grasp_plan(self, object_pose):
        """
        Request grasp pose from planning service.

        Args:
            object_pose: Detected object pose

        Returns:
            grasp_pose: Computed grasp pose, or None if planning failed
        """
        # Create service request
        request = ComputeGrasp.Request()
        request.object_pose = object_pose

        # Call service (blocking)
        future = self.grasp_client.call_async(request)
        rclpy.spin_until_future_complete(self, future)

        if future.result() is not None:
            return future.result().grasp_pose
        else:
            self.get_logger().error('Grasp planning service failed')
            return None

    def execute_task(self):
        """Main control loop executing state machine."""
        rate = self.create_rate(10)  # 10 Hz

        while rclpy.ok():
            # Execute current state behavior
            event = self.state_machine.execute_state_action(
                perception_data={'latest_image': self.latest_image},
                robot_interface=self
            )

            # Transition based on event
            if event is not None:
                self.state_machine.transition(event)

            # Check for completion or error
            if self.state_machine.state == State.COMPLETE:
                self.get_logger().info('Task completed successfully')
                break
            elif self.state_machine.state == State.ERROR:
                self.get_logger().error('Task failed')
                break

            rate.sleep()
```

### 4.3 Data Flow Architecture

Trace information flow through the system:

```
Camera → Image → Object Detection → Object Pose → Coordinate Transform
   ↓
Robot State → State Estimation → State Machine → Action Selection
   ↓
Grasp Planner → Trajectory → Motion Controller → Joint Commands → Robot
```

**Design Principles:**
- Minimize latency in critical paths
- Ensure data consistency (use timestamps, synchronization)
- Handle missing data gracefully (timeouts, defaults)

### 4.4 Testing and Validation Strategies

<div class="theory-concept">

**Unit Testing:** Test individual components in isolation
- Mock interfaces to other components
- Example: Test object detection on labeled image dataset

**Integration Testing:** Test component interactions
- Example: Verify coordinate transformations propagate correctly

**System Testing:** Test end-to-end task execution
- Example: Run full sorting task in simulation

**Robustness Testing:** Test failure modes
- Inject perception errors, communication failures
- Verify graceful degradation

</div>

**Example: Simulation-Based Testing**

```python
class SimulatedEnvironment:
    """
    Simulated environment for testing autonomous manipulation.

    Provides ground truth and controllable failure injection.
    """
    def __init__(self):
        self.objects = []
        self.robot_pose = np.array([0, 0, 0])

    def spawn_object(self, label, position, orientation):
        """Add object to simulated scene."""
        obj = {
            'label': label,
            'position': position,
            'orientation': orientation,
            'id': len(self.objects)
        }
        self.objects.append(obj)
        return obj['id']

    def get_camera_view(self, add_noise=False):
        """
        Render camera view with all visible objects.

        Args:
            add_noise: If True, add noise to simulate sensor error

        Returns:
            detections: List of detected objects
        """
        detections = []

        for obj in self.objects:
            # Simulate detection
            det = {
                'label': obj['label'],
                'position': obj['position'].copy(),
                'orientation': obj['orientation'].copy(),
                'confidence': 0.95
            }

            # Add noise if requested
            if add_noise:
                det['position'] += np.random.normal(0, 0.01, size=3)
                det['confidence'] += np.random.normal(0, 0.1)
                det['confidence'] = np.clip(det['confidence'], 0, 1)

            detections.append(det)

        return detections

    def execute_grasp(self, target_id, grasp_pose):
        """
        Simulate grasp execution.

        Returns:
            success: True if grasp succeeds
        """
        # Check if target object exists
        target = next((obj for obj in self.objects if obj['id'] == target_id), None)
        if target is None:
            return False

        # Compute distance between grasp pose and object
        distance = np.linalg.norm(grasp_pose[:3] - target['position'])

        # Grasp succeeds if close enough
        success = distance < 0.05  # 5cm threshold

        if success:
            # Remove object from scene (now in gripper)
            self.objects = [obj for obj in self.objects if obj['id'] != target_id]

        return success
```

### 4.5 Deployment Considerations

Transitioning from development to deployment:

**Reliability:**
- Automatic restart on crash
- Logging and diagnostics
- Watchdog timers for hung processes

**Safety:**
- Emergency stop integration
- Workspace monitoring (e.g., human detection)
- Graceful shutdown procedures

**Maintainability:**
- Configuration management (parameter files)
- Version control and deployment automation
- Remote monitoring and updates

### Module 4 Quiz

1. **Layered Architecture:** Why is a layered architecture beneficial for robotic systems? How does it facilitate testing and deployment?

2. **Communication Patterns:** When should you use a topic vs. a service in ROS2? Give examples of each from a manipulation system.

3. **Testing Strategy:** Design a testing plan for an autonomous sorting system. What would you test in simulation vs. on real hardware?

4. **Failure Injection:** How can you use simulation to test robustness to perception errors? Describe specific failure modes to inject.

---

## Module 5: Performance Analysis and Optimization

### 5.1 Performance Metrics

Quantifying autonomous system performance:

<div class="theory-concept">

**Task-Level Metrics:**
- **Success Rate:** $\frac{\text{# successful trials}}{\text{# total trials}}$
- **Completion Time:** Average time to complete successful trials
- **Efficiency:** $\frac{\text{optimal time}}{\text{actual time}}$

**Component-Level Metrics:**
- **Perception Accuracy:** Detection precision, recall, pose error
- **Planning Success:** % of plans that execute without collision
- **Control Error:** Tracking error during execution

**System-Level Metrics:**
- **Throughput:** Tasks completed per hour
- **Reliability:** Mean time between failures (MTBF)
- **Robustness:** Performance degradation under noise/disturbances

</div>

**Measurement Best Practices:**
- Run multiple trials (≥30 for statistical significance)
- Test in diverse conditions (lighting, object positions, etc.)
- Report mean, standard deviation, and confidence intervals

### 5.2 Bottleneck Analysis

Identify performance bottlenecks:

**Latency Profiling:**
```
Total cycle time: 500 ms
├── Image acquisition: 33 ms (camera framerate)
├── Object detection: 200 ms (neural network inference)
├── Grasp planning: 150 ms (motion planning)
├── Trajectory execution: 100 ms (motion control)
└── State machine overhead: 17 ms
```

The bottleneck is object detection (200 ms). Optimization strategies:
- Use faster detection model (MobileNet vs. ResNet)
- Reduce input resolution
- Run detection on GPU
- Run detection in parallel with motion execution

### 5.3 Failure Mode and Effects Analysis (FMEA)

Systematic analysis of potential failures:

| Component | Failure Mode | Effect | Likelihood | Severity | Mitigation |
|-----------|--------------|--------|------------|----------|------------|
| Camera | Calibration drift | Pose error | Medium | High | Periodic recalibration |
| Detector | False negative | Object missed | Low | Medium | Multiple viewpoints |
| Grasp Planner | Infeasible plan | Motion failure | Medium | High | Collision checking |
| Gripper | Grasp slip | Object dropped | High | High | Force sensing verification |

Prioritize mitigation for high likelihood × high severity failures.

### 5.4 Optimization Strategies

<div class="alternative-approach">

**Perception Optimization:**
- **Region of Interest:** Only process workspace area
- **Multi-Resolution:** High resolution for detection, low for tracking
- **Temporal Coherence:** Track objects across frames rather than re-detect

**Planning Optimization:**
- **Caching:** Reuse previously computed plans for similar scenarios
- **Lazy Evaluation:** Plan only when needed, not preemptively
- **Hierarchical Planning:** Coarse plan first, refine locally

**Control Optimization:**
- **Trajectory Parameterization:** Use compact representations (splines, DMPs)
- **Feedforward Control:** Precompute control inputs for nominal trajectory
- **Parallel Execution:** Execute next plan while current motion continues

</div>

### 5.5 Scalability Analysis

How does performance scale with task complexity?

**Factors:**
- Number of objects (affects detection time, planning complexity)
- Workspace size (affects motion planning time)
- Task constraints (affects planning feasibility)

**Example Scaling:**
- 2 objects: 95% success, 30s average time
- 5 objects: 85% success, 90s average time
- 10 objects: 70% success, 200s average time

Understanding scalability informs system design decisions and application domains.

### Module 5 Quiz

1. **Metrics Design:** For a warehouse picking robot, design a set of performance metrics. Which metrics matter most for deployment?

2. **Bottleneck Analysis:** Given a system where object detection takes 80% of cycle time, propose three optimization strategies. Estimate the potential speedup for each.

3. **FMEA:** Conduct an FMEA for a coffee-making robot. Identify at least 5 failure modes and propose mitigations.

4. **Scalability:** How would you test the scalability of your sorting system? Design an experimental protocol to measure performance vs. number of objects.

---

## Summary

This capstone course has examined the principles of integrating perception, planning, and control into cohesive autonomous systems. Key insights include:

**System Architecture:**
- Hierarchical organization separates deliberative planning from reactive control
- State machines and behavior trees formalize task logic
- Modular design enables testing, debugging, and maintenance

**Integration Challenges:**
- Coordinate frame transformations are essential for perception-action coupling
- Uncertainty in perception must be handled through filtering and verification
- Communication patterns (topics, services) structure information flow

**Decision-Making:**
- Task planning and motion planning address complementary problems
- Hybrid reactive-deliberative approaches balance flexibility and responsiveness
- Failure detection and recovery are essential for robustness

**Performance Engineering:**
- Systematic metrics quantify task success and system performance
- Bottleneck analysis guides optimization efforts
- FMEA identifies and prioritizes failure mitigations

**Practical Deployment:**
- Simulation enables rapid testing and failure injection
- Layered architecture facilitates portability and testing
- Real-world deployment requires safety, reliability, and maintainability features

The capstone project synthesizes these concepts, requiring students to design, implement, and evaluate a complete autonomous manipulation system. This experience bridges theory and practice, preparing students for careers in autonomous robotics.

---

## Capstone Laboratory Project

### Capstone Project: Autonomous Block Sorting System

<div class="optional-exercise">

**Objective:** Design and implement a complete autonomous system that sorts colored blocks into matching containers.

**Duration:** 6-8 hours (full day intensive)

**Task Specification:**

The system must:
1. Detect colored blocks (red and blue) on a table
2. Detect corresponding colored containers
3. Pick each block
4. Place it in the matching colored container
5. Operate autonomously after a single start command

**Success Criteria:**
- Successfully sort both blocks in ≥80% of trials
- Complete task in <120 seconds per trial
- Gracefully handle failure modes (display error message, safe stop)
- Well-structured code with clear module boundaries

</div>

### Phase 1: System Architecture Design (60 minutes)

<div class="optional-exercise">

**Tasks:**

1. **Architecture Diagram**
   - Draw block diagram showing all components
   - Define interfaces between components
   - Identify data flow paths

2. **State Machine Design**
   - Design FSM or behavior tree for task logic
   - Include all states, transitions, and events
   - Add error states and recovery transitions

3. **Module Specification**
   - List all software modules/nodes
   - Define responsibility of each module
   - Specify topics, services, and parameters

4. **Coordinate Frames**
   - Identify all coordinate frames
   - Define transformation chain
   - Plan calibration procedure

**Deliverable:** System architecture document with diagrams and specifications.

</div>

<div class="exercise-tip">

**Design Tip:** Start simple. A working system with basic functionality is better than an ambitious design that doesn't run. You can add features incrementally after the core pipeline works.

</div>

### Phase 2: Perception Subsystem (90 minutes)

<div class="optional-exercise">

**Tasks:**

1. **Object Detection**
   - Implement or integrate object detection (color-based or learned)
   - Detect blocks and containers
   - Filter by color (red vs. blue)
   - Validate detections (workspace bounds, confidence threshold)

2. **Pose Estimation**
   - Estimate 3D positions from RGB-D data
   - Compute orientations (or assume canonical)
   - Transform to robot base frame

3. **Testing and Validation**
   - Test detection on multiple scenes
   - Measure detection accuracy
   - Verify coordinate transformations

4. **Robustness Features**
   - Handle occlusions
   - Reject outliers
   - Temporal filtering (optional)

**Deliverable:** Perception module that reliably detects and localizes blocks and containers.

</div>

<div class="exercise-advanced">

**Advanced Extension:** Implement multi-view perception by fusing detections from multiple cameras. Compare single-view vs. multi-view robustness.

</div>

### Phase 3: Planning and Control (90 minutes)

<div class="optional-exercise">

**Tasks:**

1. **Grasp Computation**
   - Define grasp poses relative to object frames
   - Compute grasp approach and retract poses
   - Check reachability

2. **Motion Planning**
   - Integrate motion planning library (MoveIt, or simple IK)
   - Plan collision-free trajectories
   - Handle planning failures gracefully

3. **Trajectory Execution**
   - Send trajectories to robot controller
   - Monitor execution progress
   - Detect motion failures (timeout, collision)

4. **Gripper Control**
   - Implement open/close commands
   - Verify grasp success (force sensing or vision)

**Deliverable:** Motion planning and control module that executes pick-and-place primitives.

</div>

### Phase 4: Integration and State Machine (90 minutes)

<div class="optional-exercise">

**Tasks:**

1. **State Machine Implementation**
   - Implement FSM or behavior tree from Phase 1 design
   - Integrate perception and planning modules
   - Add state transition logic

2. **Object Selection Logic**
   - Prioritize which object to pick next
   - Handle multiple objects of same color
   - Update after each successful placement

3. **Failure Handling**
   - Detect perception failures (no objects found)
   - Detect motion failures (planning/execution errors)
   - Implement retry and abort logic

4. **Logging and Debugging**
   - Add comprehensive logging
   - Visualize state transitions
   - Record timing information

**Deliverable:** Fully integrated system executing the sorting task autonomously.

</div>

<div class="exercise-tip">

**Integration Tip:** Test each state transition individually before running the full state machine. Use mock data or manual triggers to isolate component issues.

</div>

### Phase 5: Testing and Evaluation (60 minutes)

<div class="optional-exercise">

**Tasks:**

1. **Functional Testing**
   - Run end-to-end task 10 times
   - Record success rate, completion time
   - Identify failure modes

2. **Robustness Testing**
   - Vary object positions
   - Test different lighting conditions
   - Introduce disturbances (optional)

3. **Performance Analysis**
   - Profile cycle time breakdown
   - Identify bottlenecks
   - Propose optimizations

4. **Failure Analysis**
   - Classify failures by type (perception, planning, execution)
   - Analyze root causes
   - Implement mitigations

**Deliverable:** Evaluation report with quantitative metrics and failure analysis.

</div>

### Phase 6: Documentation and Demonstration (60 minutes)

<div class="optional-exercise">

**Tasks:**

1. **Code Documentation**
   - Add docstrings to all functions/classes
   - Comment complex logic
   - Create README with setup instructions

2. **System Documentation**
   - Describe architecture and design decisions
   - Document interfaces and parameters
   - Include state machine diagram

3. **Demo Preparation**
   - Prepare demonstration scenario
   - Record video of successful execution
   - Prepare slides explaining approach

4. **Reflection**
   - What worked well?
   - What were the main challenges?
   - What would you do differently?

**Deliverable:** Complete documentation package and demonstration.

</div>

---

## Certification Requirements

To receive certification for AI-205 and complete the AI Engineering Track, students must:

1. **Theoretical Mastery:** Score at least 80% on all module quizzes
2. **Capstone Project:** Successfully complete all phases of the autonomous sorting system:
   - Working perception module with ≥90% detection accuracy
   - Functional motion planning and control
   - Integrated state machine executing full task
   - ≥80% success rate over 10 trials
   - Complete documentation and demonstration
3. **Final Presentation:** 10-minute presentation covering:
   - System architecture and design rationale
   - Key technical challenges and solutions
   - Performance evaluation and failure analysis
   - Lessons learned and future improvements

The capstone project demonstrates mastery of the entire AI robotics pipeline: perception, learning, planning, and integration. Successful completion qualifies students as AI Robotics Engineers capable of designing and deploying autonomous systems.

---

## Further Reading

**Systems Integration:**
- Murphy, R. R. (2000). *Introduction to AI Robotics*. MIT Press.
- Bekey, G. A. (2005). *Autonomous Robots: From Biological Inspiration to Implementation and Control*. MIT Press.

**Task and Motion Planning:**
- Kaelbling, L. P., & Lozano-Pérez, T. (2013). "Integrated task and motion planning in belief space." *IJRR*.
- Garrett, C. R., et al. (2021). "Integrated task and motion planning." *Annual Review of Control, Robotics, and Autonomous Systems*.

**Behavior Trees and State Machines:**
- Colledanchise, M., & Ögren, P. (2018). *Behavior Trees in Robotics and AI*. CRC Press.
- Iovino, M., et al. (2022). "A survey of behavior trees in robotics and AI." *Robotics and Autonomous Systems*.

**Software Architecture:**
- Brugali, D., & Shakhimardanov, A. (2010). "Component-based robotic engineering." *IEEE Robotics & Automation Magazine*.
- Quigley, M., et al. (2009). "ROS: An open-source Robot Operating System." *ICRA Workshop*.

**Failure Recovery:**
- Pettersson, O. (2005). "Execution monitoring in robotics: A survey." *Robotics and Autonomous Systems*.
- Beetz, M., et al. (2010). "CRAM—A cognitive robot abstract machine for everyday manipulation in human environments." *IROS*.

**Testing and Validation:**
- Afzal, A., et al. (2020). "A study on the challenges of using robotics simulators for testing." *Information and Software Technology*.
- Luckcuck, M., et al. (2019). "Formal specification and verification of autonomous robotic systems: A survey." *ACM Computing Surveys*.
