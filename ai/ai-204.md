---
id: ai-204
title: "AI-204: Reinforcement Learning for Robotics"
sidebar_position: 4
version: 2.0.0
link: https://raw.githubusercontent.com/Academy-of-Robotic-Sciences/courses/main/course-styles.css
---

# AI-204: Reinforcement Learning for Robotics

## Course Overview

| | |
|---|---|
| **Course Code** | AI-204 |
| **Duration** | 8 hours |
| **Level** | Advanced |
| **Prerequisites** | AI-201, AI-203, Calculus, Probability Theory |
| **Format** | Academic lecture with optional laboratory exercises |
| **Theory/Practice** | 60% theoretical foundations / 40% optional hands-on labs |

## Learning Objectives

### Theoretical Objectives
- Master the mathematical foundations of reinforcement learning theory
- Analyze Markov Decision Processes and their solution methods
- Understand value functions, Bellman equations, and dynamic programming
- Examine policy gradient methods and actor-critic architectures
- Evaluate deep reinforcement learning algorithms and their convergence properties

### Practical Objectives (Optional Labs)
- Implement tabular Q-learning from first principles
- Build Deep Q-Networks for continuous state spaces
- Train policy gradient agents for robotic control tasks
- Deploy trained RL agents in simulation environments

---

## Module 1: Foundations of Reinforcement Learning

### 1.1 Historical Context

<div class="historical-note">

Reinforcement learning has deep roots in psychology, control theory, and artificial intelligence. The concept of learning through reward and punishment dates to Thorndike's "Law of Effect" (1911) and Skinner's operant conditioning research (1938).

The modern mathematical framework emerged in the 1950s with Bellman's work on dynamic programming and the formulation of Markov Decision Processes. The temporal difference learning algorithm was introduced by Sutton (1988), providing a bridge between Monte Carlo methods and dynamic programming.

The deep learning revolution transformed RL in 2013 when Mnih et al. demonstrated that Deep Q-Networks could master Atari games from raw pixel inputs, achieving superhuman performance. This breakthrough opened the era of deep reinforcement learning, culminating in systems like AlphaGo (2016) and robotic manipulation breakthroughs in the 2020s.

</div>

Reinforcement learning addresses the fundamental problem of learning optimal behavior through interaction with an environment. Unlike supervised learning, there are no explicit labels—only rewards that signal success or failure. Unlike imitation learning, the agent must discover effective strategies through exploration and experimentation.

### 1.2 The Markov Decision Process Framework

The mathematical foundation of reinforcement learning is the Markov Decision Process (MDP), a formal model of sequential decision-making under uncertainty.

<div class="theory-concept">

**Definition 1.1 (Markov Decision Process):** An MDP is defined by the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \rho_0, \gamma)$ where:

- $\mathcal{S}$ is the state space (possibly infinite)
- $\mathcal{A}$ is the action space (possibly continuous)
- $\mathcal{T}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]$ is the transition probability function satisfying $\int_{s' \in \mathcal{S}} \mathcal{T}(s' | s, a) ds' = 1$
- $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function (bounded: $|r(s,a)| \leq R_{\max}$)
- $\rho_0: \mathcal{S} \rightarrow [0, 1]$ is the initial state distribution
- $\gamma \in [0, 1)$ is the discount factor

The **Markov property** states that the future is independent of the past given the present:

$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0) = P(s_{t+1} | s_t, a_t) = \mathcal{T}(s_{t+1} | s_t, a_t)$$

</div>

A **policy** $\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ defines the agent's behavior, mapping states to probability distributions over actions. The goal is to find an optimal policy that maximizes expected cumulative discounted reward.

### 1.3 Value Functions and Bellman Equations

Value functions quantify the long-term desirability of states and state-action pairs under a given policy.

<div class="theory-concept">

**Definition 1.2 (Value Functions):**

The **state-value function** $V^\pi(s)$ gives the expected return starting from state $s$ and following policy $\pi$:

$$V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 = s \right]$$

The **action-value function** $Q^\pi(s, a)$ gives the expected return starting from state $s$, taking action $a$, then following policy $\pi$:

$$Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 = s, a_0 = a \right]$$

</div>

These functions satisfy recursive relationships known as the Bellman equations:

<div class="theory-concept">

**Theorem 1.1 (Bellman Expectation Equations):**

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[ r(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{T}(s'|s,a) V^\pi(s') \right]$$

$$Q^\pi(s,a) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{T}(s'|s,a) \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s', a')$$

These equations express the fundamental principle: the value of a state equals the immediate reward plus the discounted value of successor states.

</div>

### 1.4 Optimal Value Functions and Policies

The optimal value functions satisfy the Bellman optimality equations:

<div class="theory-concept">

**Theorem 1.2 (Bellman Optimality Equations):**

$$V^*(s) = \max_{a \in \mathcal{A}} \left[ r(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{T}(s'|s,a) V^*(s') \right]$$

$$Q^*(s,a) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{T}(s'|s,a) \max_{a' \in \mathcal{A}} Q^*(s', a')$$

The optimal policy can be recovered from the optimal action-value function:

$$\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)$$

**Uniqueness:** While there may be multiple optimal policies, the optimal value function $V^*$ is unique.

</div>

### 1.5 The Agent-Environment Interaction Loop

Reinforcement learning unfolds as a sequential interaction process:

1. Agent observes state $s_t \in \mathcal{S}$
2. Agent selects action $a_t \sim \pi(\cdot | s_t)$
3. Environment transitions to $s_{t+1} \sim \mathcal{T}(\cdot | s_t, a_t)$
4. Agent receives reward $r_t = r(s_t, a_t)$
5. Repeat

**Example: Simple RL Environment**

```python
import numpy as np

class SimpleEnvironment:
    """
    Minimal RL environment implementing the Gym interface.

    State space: Continuous R^n
    Action space: Discrete {0, 1, ..., k-1}
    """
    def __init__(self, state_dim=4, num_actions=2):
        self.state_dim = state_dim
        self.num_actions = num_actions
        self.state = None

    def reset(self):
        """Initialize a new episode."""
        self.state = np.random.randn(self.state_dim)
        return self.state

    def step(self, action):
        """
        Execute action and return (next_state, reward, done, info).

        Args:
            action: Integer in [0, num_actions)

        Returns:
            next_state: New state vector
            reward: Scalar reward
            done: Boolean episode termination flag
            info: Dictionary of auxiliary information
        """
        # Simplified dynamics: state evolves based on action
        noise = 0.1 * np.random.randn(self.state_dim)
        self.state = 0.9 * self.state + 0.1 * action + noise

        # Reward based on distance to origin
        reward = -np.linalg.norm(self.state)

        # Episode ends if state norm exceeds threshold
        done = np.linalg.norm(self.state) > 5.0

        return self.state, reward, done, {}
```

### Module 1 Quiz

1. **Markov Property:** Explain why the Markov property is essential for the MDP framework. Give an example of a robotics problem that would violate the Markov property if state is defined incorrectly.

2. **Bellman Equations:** Derive the Bellman expectation equation for $V^\pi(s)$ from the definition of the value function. What does this equation tell us about the relationship between current and future values?

3. **Discount Factor:** How does the discount factor $\gamma$ affect the agent's behavior? Compare $\gamma = 0.9$ vs. $\gamma = 0.99$ in terms of planning horizon and convergence properties.

4. **Optimality:** Prove that if $Q^*$ satisfies the Bellman optimality equation, then the greedy policy $\pi(s) = \arg\max_a Q^*(s,a)$ is optimal.

---

## Module 2: Dynamic Programming and Tabular Methods

### 2.1 Dynamic Programming Approach

When the MDP model (transition probabilities and rewards) is fully known, dynamic programming provides exact methods for computing optimal policies.

<div class="theory-concept">

**Algorithm 2.1 (Value Iteration):**

Initialize $V(s)$ arbitrarily for all $s \in \mathcal{S}$

Repeat until convergence:
$$V_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left[ r(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{T}(s'|s,a) V_k(s') \right]$$

**Convergence Guarantee:** Under the assumption that $\gamma < 1$ and $|\mathcal{S}|, |\mathcal{A}| < \infty$, value iteration converges to $V^*$ at a geometric rate.

</div>

Policy iteration alternates between policy evaluation and policy improvement:

<div class="theory-concept">

**Algorithm 2.2 (Policy Iteration):**

1. **Initialization:** Start with arbitrary policy $\pi_0$
2. **Policy Evaluation:** Solve for $V^{\pi_k}$ using the Bellman expectation equation
3. **Policy Improvement:** Update policy greedily:
   $$\pi_{k+1}(s) = \arg\max_{a} \left[ r(s,a) + \gamma \sum_{s'} \mathcal{T}(s'|s,a) V^{\pi_k}(s') \right]$$
4. **Repeat** until $\pi_{k+1} = \pi_k$

Policy iteration often converges in fewer iterations than value iteration, though each iteration is more expensive.

</div>

### 2.2 Temporal Difference Learning

In most real-world problems, the transition dynamics $\mathcal{T}$ are unknown. Temporal Difference (TD) learning estimates value functions from experience without requiring a model.

<div class="theory-concept">

**Algorithm 2.3 (TD(0) Learning):**

Initialize $V(s)$ arbitrarily

For each episode:
- Observe $s_t$, take action $a_t$, observe $r_t$, $s_{t+1}$
- Update: $$V(s_t) \leftarrow V(s_t) + \alpha [r_t + \gamma V(s_{t+1}) - V(s_t)]$$

The **TD error** $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ measures the difference between the estimated value and the observed return.

</div>

TD learning combines advantages of Monte Carlo (learning from experience) and dynamic programming (bootstrapping from current estimates):

- **Bootstrap:** Updates based on estimates rather than complete returns
- **Online:** Can learn during the episode rather than waiting for termination
- **Model-free:** Doesn't require knowledge of transition dynamics

### 2.3 Q-Learning

Q-learning learns the optimal action-value function directly from experience, without needing a model of the environment.

<div class="theory-concept">

**Algorithm 2.4 (Q-Learning):**

Initialize $Q(s, a)$ arbitrarily for all $s, a$

For each step:
1. Observe state $s_t$
2. Select action $a_t$ (e.g., $\epsilon$-greedy)
3. Execute $a_t$, observe $r_t$ and $s_{t+1}$
4. Update:
   $$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]$$

**Off-policy Property:** Q-learning is off-policy because it updates based on the maximum Q-value (greedy action) regardless of which action was actually taken.

</div>

<div class="theory-concept">

**Theorem 2.1 (Q-Learning Convergence):** Under the conditions:
1. All state-action pairs are visited infinitely often
2. Learning rate satisfies $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$
3. Rewards are bounded

Q-learning converges to $Q^*$ with probability 1.

</div>

**Example: Tabular Q-Learning Implementation**

```python
import numpy as np
from collections import defaultdict

class QLearningAgent:
    """
    Tabular Q-learning agent for discrete state/action spaces.
    """
    def __init__(self, num_actions, learning_rate=0.1, gamma=0.99, epsilon=0.1):
        self.num_actions = num_actions
        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon

        # Q-table: dictionary mapping (state, action) to value
        self.Q = defaultdict(float)

    def get_action(self, state, training=True):
        """
        Epsilon-greedy action selection.

        Args:
            state: Current state (must be hashable)
            training: If False, always act greedily
        """
        if training and np.random.random() < self.epsilon:
            # Explore: random action
            return np.random.randint(self.num_actions)
        else:
            # Exploit: greedy action
            q_values = [self.Q[(state, a)] for a in range(self.num_actions)]
            return np.argmax(q_values)

    def update(self, state, action, reward, next_state, done):
        """
        Q-learning update rule.

        Args:
            state: Current state
            action: Action taken
            reward: Reward received
            next_state: Resulting state
            done: Whether episode terminated
        """
        # Current Q-value
        current_q = self.Q[(state, action)]

        # TD target
        if done:
            target = reward
        else:
            max_next_q = max([self.Q[(next_state, a)]
                             for a in range(self.num_actions)])
            target = reward + self.gamma * max_next_q

        # TD update
        td_error = target - current_q
        self.Q[(state, action)] = current_q + self.lr * td_error
```

### 2.4 Exploration vs. Exploitation

A fundamental challenge in RL is balancing exploration (trying new actions to discover their value) with exploitation (choosing actions known to be good).

<div class="theory-concept">

**$\epsilon$-Greedy Policy:**

$$\pi(a|s) = \begin{cases}
1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|} & \text{if } a = \arg\max_{a'} Q(s, a') \\
\frac{\epsilon}{|\mathcal{A}|} & \text{otherwise}
\end{cases}$$

With probability $1-\epsilon$, select the greedy action; otherwise, select uniformly at random.

</div>

<div class="alternative-approach">

**Optimistic Initialization:** Initialize $Q(s,a)$ to high values, encouraging exploration of under-visited state-action pairs. The agent will naturally explore since actual returns will be lower than optimistic estimates.

**Upper Confidence Bound (UCB):** Select actions based on:
$$a_t = \arg\max_a \left[ Q(s_t, a) + c \sqrt{\frac{\ln t}{N(s_t, a)}} \right]$$

where $N(s_t, a)$ is the visit count. This balances exploitation (high Q-value) with uncertainty (low visit count).

**Boltzmann Exploration:** Sample actions according to:
$$\pi(a|s) = \frac{\exp(Q(s,a)/\tau)}{\sum_{a'} \exp(Q(s,a')/\tau)}$$

where $\tau$ is the temperature parameter controlling exploration.

</div>

### Module 2 Quiz

1. **DP vs TD:** Compare dynamic programming and temporal difference learning. Under what circumstances is each method preferred?

2. **TD Error:** Explain the intuition behind the TD error $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$. What does it mean when this error is positive vs. negative?

3. **Q-Learning Off-Policy:** Why is Q-learning considered "off-policy"? How does this differ from SARSA (State-Action-Reward-State-Action)?

4. **Exploration Strategies:** For a robot learning to reach a target, compare $\epsilon$-greedy vs. UCB exploration. Which would you expect to be more sample-efficient and why?

---

## Module 3: Deep Reinforcement Learning

### 3.1 Function Approximation Necessity

Tabular methods are limited to small, discrete state spaces. For robotics problems with continuous states (joint angles, velocities, images), function approximation is essential.

Consider a vision-based manipulation task: a 64×64 RGB image has $256^{64 \times 64 \times 3} \approx 10^{30000}$ possible states—impossible to store in a table. Neural networks provide a practical solution by learning to generalize across similar states.

### 3.2 Deep Q-Networks (DQN)

DQN, introduced by Mnih et al. (2013), combines Q-learning with deep neural networks to handle high-dimensional observations.

<div class="theory-concept">

**DQN Architecture:**

Replace the tabular Q-function with a neural network $Q(s, a; \theta)$ parameterized by weights $\theta$.

**Loss Function:**
$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]$$

where:
- $\mathcal{D}$ is the replay buffer containing past experiences
- $\theta^-$ are target network parameters (periodically updated from $\theta$)

</div>

Two key innovations stabilize training:

**1. Experience Replay:** Store transitions $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer and sample mini-batches uniformly for updates. This breaks temporal correlations and improves data efficiency.

**2. Target Network:** Maintain a separate target network with parameters $\theta^-$ that are updated slowly. This stabilizes the TD target, preventing harmful feedback loops.

**Example: DQN Implementation**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

class DQNNetwork(nn.Module):
    """
    Deep Q-Network for continuous state spaces.
    """
    def __init__(self, state_dim, num_actions, hidden_dim=128):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_actions)
        )

    def forward(self, state):
        """
        Compute Q-values for all actions.

        Args:
            state: Tensor of shape (batch, state_dim)
        Returns:
            q_values: Tensor of shape (batch, num_actions)
        """
        return self.network(state)


class DQNAgent:
    """
    Deep Q-Learning agent with experience replay and target network.
    """
    def __init__(self, state_dim, num_actions,
                 learning_rate=1e-3, gamma=0.99, epsilon=0.1,
                 buffer_size=10000, batch_size=64, target_update_freq=100):

        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq

        # Q-network and target network
        self.q_network = DQNNetwork(state_dim, num_actions)
        self.target_network = DQNNetwork(state_dim, num_actions)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Replay buffer
        self.replay_buffer = deque(maxlen=buffer_size)

        # Training step counter
        self.step_count = 0

    def get_action(self, state, training=True):
        """Epsilon-greedy action selection."""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.num_actions - 1)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_network(state_tensor)
            return q_values.argmax(dim=1).item()

    def store_transition(self, state, action, reward, next_state, done):
        """Add transition to replay buffer."""
        self.replay_buffer.append((state, action, reward, next_state, done))

    def update(self):
        """
        Perform one DQN update step.
        Returns: Loss value (or None if buffer too small)
        """
        if len(self.replay_buffer) < self.batch_size:
            return None

        # Sample mini-batch
        batch = random.sample(self.replay_buffer, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # Convert to tensors
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)

        # Current Q-values
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()

        # Target Q-values (using target network)
        with torch.no_grad():
            max_next_q = self.target_network(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)

        # Compute loss and update
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10)
        self.optimizer.step()

        # Update target network periodically
        self.step_count += 1
        if self.step_count % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        return loss.item()
```

### 3.3 Challenges in Deep RL

Deep reinforcement learning faces several fundamental challenges:

**1. Sample Inefficiency:** DQN typically requires millions of environment interactions. For robotics, where real-world data is expensive, this is prohibitive.

**2. Instability:** The combination of function approximation, bootstrapping, and off-policy learning can cause divergence. The target value is constantly changing as the network updates.

**3. Overestimation Bias:** The max operator in Q-learning leads to systematic overestimation of action values due to noise.

<div class="alternative-approach">

**Double DQN:** Decouple action selection from evaluation to reduce overestimation:

$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, \arg\max_{a'} Q(s_{t+1}, a'; \theta); \theta^-) - Q(s_t, a_t) \right]$$

Action is selected using the online network $\theta$, but evaluated using the target network $\theta^-$.

</div>

### 3.4 Extensions and Improvements

Several architectural improvements enhance DQN performance:

**Dueling DQN:** Separate the Q-function into value and advantage streams:

$$Q(s, a; \theta) = V(s; \theta_V) + A(s, a; \theta_A) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta_A)$$

This architecture learns which states are valuable independent of the action choice.

**Prioritized Experience Replay:** Sample transitions with probability proportional to their TD error:

$$P(i) \propto |r_i + \gamma \max_{a'} Q(s_{i+1}, a'; \theta^-) - Q(s_i, a_i; \theta)|^\alpha$$

High-error transitions (where the network is "surprised") are replayed more frequently, accelerating learning.

**Noisy Networks:** Replace $\epsilon$-greedy exploration with learned stochastic policies by adding parametric noise to network weights.

### Module 3 Quiz

1. **Function Approximation:** Why does the combination of bootstrapping, off-policy learning, and function approximation create instability? How do target networks help?

2. **Experience Replay:** Explain how experience replay addresses the problem of correlated samples. What are the trade-offs compared to on-policy learning?

3. **Overestimation:** Derive why the max operator in Q-learning leads to overestimation bias. How does Double DQN address this?

4. **Architecture Design:** When would the dueling architecture provide the most benefit? Give a robotics example where separating value and advantage would be particularly useful.

---

## Module 4: Policy Gradient Methods

### 4.1 Limitations of Value-Based Methods

While Q-learning and DQN are powerful, they have fundamental limitations:

1. **Discrete Actions Only:** The max operator requires enumerating all actions, which is infeasible for continuous action spaces (e.g., joint torques).
2. **Deterministic Policies:** Value-based methods typically learn deterministic policies, which can be suboptimal in partially observable environments.
3. **Small Changes in Values, Large Changes in Policy:** A tiny change in Q-values can cause abrupt policy changes, leading to instability.

Policy gradient methods directly parameterize and optimize the policy $\pi_\theta(a|s)$.

### 4.2 The Policy Gradient Theorem

<div class="theory-concept">

**Objective Function:** The goal is to maximize expected return:

$$J(\theta) = \mathbb{E}_{s_0 \sim \rho_0, \tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right]$$

where $\tau = (s_0, a_0, s_1, a_1, ...)$ is a trajectory sampled from the policy.

**Theorem 4.1 (Policy Gradient Theorem):** The gradient of $J(\theta)$ with respect to policy parameters is:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t'=t}^\infty \gamma^{t'} r(s_{t'}, a_{t'}) \right]$$

This remarkable result shows we can estimate the gradient from sample trajectories without knowing the environment dynamics.

</div>

The intuitive interpretation: actions that lead to high returns should have their probability increased; actions leading to low returns should be decreased.

### 4.3 REINFORCE Algorithm

<div class="theory-concept">

**Algorithm 4.1 (REINFORCE):**

Initialize policy parameters $\theta$

For each episode:
1. Generate trajectory $\tau = (s_0, a_0, r_0, ..., s_T)$ by following $\pi_\theta$
2. For each timestep $t$:
   - Compute return $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$
   - Update: $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) G_t$

**Variance Reduction:** Subtract a baseline $b(s_t)$ to reduce variance without introducing bias:
$$\nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b(s_t)) \right]$$

A common choice is $b(s_t) = V(s_t)$, the value function.

</div>

**Example: REINFORCE Implementation**

```python
class PolicyNetwork(nn.Module):
    """
    Stochastic policy network for continuous action spaces.
    Outputs mean and log-std of a Gaussian distribution.
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Returns: (mean, log_std) tensors of shape (batch, action_dim)
        """
        shared_features = self.shared(state)
        mean = self.mean_head(shared_features)
        log_std = self.log_std_head(shared_features)
        return mean, log_std

    def sample_action(self, state):
        """Sample an action from the policy."""
        mean, log_std = self.forward(state)
        std = torch.exp(log_std)
        dist = torch.distributions.Normal(mean, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1)
        return action, log_prob


class REINFORCEAgent:
    """
    REINFORCE policy gradient agent with baseline.
    """
    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)

        # Value function baseline
        self.value_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=learning_rate)

    def update(self, trajectories):
        """
        Update policy using REINFORCE.

        Args:
            trajectories: List of (states, actions, rewards, log_probs) tuples
        """
        policy_loss = 0
        value_loss = 0

        for states, actions, rewards, log_probs in trajectories:
            # Compute returns
            returns = []
            G = 0
            for r in reversed(rewards):
                G = r + self.gamma * G
                returns.insert(0, G)
            returns = torch.FloatTensor(returns)

            # Normalize returns
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

            # Compute baseline
            states_tensor = torch.FloatTensor(states)
            values = self.value_net(states_tensor).squeeze()

            # Policy gradient loss
            advantages = returns - values.detach()
            policy_loss += -(log_probs * advantages).sum()

            # Value function loss
            value_loss += nn.MSELoss()(values, returns)

        # Update policy
        self.optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)
        self.optimizer.step()

        # Update value function
        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        return policy_loss.item(), value_loss.item()
```

### 4.4 Actor-Critic Methods

Actor-critic methods combine value-based and policy-based approaches:

- **Actor:** The policy $\pi_\theta(a|s)$ that selects actions
- **Critic:** The value function $V_\phi(s)$ that evaluates states

<div class="theory-concept">

**Algorithm 4.2 (Advantage Actor-Critic):**

For each step:
1. Observe $s_t$, select $a_t \sim \pi_\theta(\cdot | s_t)$
2. Observe $r_t$, $s_{t+1}$
3. Compute TD error: $\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)$
4. Update critic: $\phi \leftarrow \phi + \alpha_\phi \delta_t \nabla_\phi V_\phi(s_t)$
5. Update actor: $\theta \leftarrow \theta + \alpha_\theta \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t)$

The TD error $\delta_t$ serves as an estimate of the advantage $A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)$.

</div>

<div class="historical-note">

Actor-critic methods date back to the 1980s, but modern variants have achieved remarkable success. Deep Deterministic Policy Gradient (DDPG, 2015) enabled continuous control for robotics. Proximal Policy Optimization (PPO, 2017) became the workhorse algorithm for many applications due to its stability and simplicity. Soft Actor-Critic (SAC, 2018) added maximum entropy reinforcement learning, improving exploration and robustness.

</div>

### 4.5 Trust Region and Proximal Methods

A challenge with policy gradients is determining appropriate step sizes. Too large a step can catastrophically degrade performance.

<div class="theory-concept">

**Trust Region Policy Optimization (TRPO):** Constrains policy updates to a trust region:

$$\max_\theta \mathbb{E}_{s,a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\theta_{\text{old}}}}(s,a) \right]$$

subject to $\mathbb{E}_{s} [D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|s) || \pi_\theta(\cdot|s))] \leq \delta$

The KL divergence constraint ensures the new policy doesn't deviate too far from the old policy.

**Proximal Policy Optimization (PPO):** Simplifies TRPO using a clipped objective:

$$L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A, \text{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\epsilon, 1+\epsilon\right) A \right) \right]$$

where $\epsilon \approx 0.2$ and $A$ is the advantage estimate. This removes large policy changes without requiring expensive constraint optimization.

</div>

### Module 4 Quiz

1. **Policy Gradients vs Q-Learning:** What are the fundamental advantages of policy gradient methods over Q-learning for robotic control tasks?

2. **Variance Reduction:** Why do policy gradients suffer from high variance? Explain how a baseline reduces variance without introducing bias.

3. **Actor-Critic:** In the actor-critic framework, what role does the critic play? How does it differ from the value function in REINFORCE?

4. **Trust Regions:** Explain why constraining policy updates with KL divergence (as in TRPO) improves learning stability. What problem does this solve?

---

## Module 5: Reinforcement Learning for Robotics

### 5.1 Challenges Specific to Robotics

Applying RL to physical robots introduces unique challenges beyond standard benchmarks:

**1. Sample Efficiency:** Real-world robot interactions are slow and expensive. An algorithm requiring millions of samples is impractical.

**2. Safety:** Random exploration can damage robots or their environment. Safe exploration strategies are essential.

**3. Continuous High-Dimensional Actions:** Robot control involves continuous actions (joint torques, velocities) in high-dimensional spaces (7-DOF arm = 7-dimensional action space).

**4. Partial Observability:** Sensors provide incomplete information. The Markov property may not hold without careful state representation.

**5. Sim-to-Real Gap:** Policies trained in simulation often fail on real hardware due to model inaccuracies.

### 5.2 Sim-to-Real Transfer for RL

<div class="theory-concept">

**Domain Randomization:** Train the policy over a distribution of simulation parameters:

$$J(\theta) = \mathbb{E}_{\xi \sim \mathcal{P}(\Xi)} \left[ \mathbb{E}_{\tau \sim \pi_\theta, \mathcal{M}_\xi} \left[ \sum_t \gamma^t r_t \right] \right]$$

where $\xi$ represents simulation parameters (friction, mass, visual appearance) sampled from distribution $\mathcal{P}(\Xi)$.

By training on diverse simulated environments, the policy learns to be robust to modeling errors.

</div>

**System Identification:** Use small amounts of real-world data to identify simulation parameters that match reality:

$$\xi^* = \arg\min_\xi D(\text{RealData}, \text{SimData}(\xi))$$

Then fine-tune the policy in the identified simulator.

**Progressive Sim-to-Real:** Gradually increase task difficulty and realism:
1. Train in simple simulation
2. Transfer to higher-fidelity simulation
3. Fine-tune with limited real-world data

### 5.3 Model-Based Reinforcement Learning

Model-based RL learns a dynamics model $\hat{\mathcal{T}}(s' | s, a)$ and uses it for planning or to generate synthetic data.

<div class="theory-concept">

**Model-Based RL Framework:**

1. **Data Collection:** Execute policy in environment, collect $(s_t, a_t, s_{t+1})$ transitions
2. **Model Learning:** Train dynamics model $\hat{s}_{t+1} = f_\psi(s_t, a_t)$ using supervised learning
3. **Policy Optimization:** Use the learned model for:
   - **Planning:** Model Predictive Control (MPC) with the learned model
   - **Data Augmentation:** Generate synthetic rollouts for policy training

</div>

**Advantages:**
- Sample efficiency: Model can be queried "for free" after learning
- Transfer: Models learned on one task may generalize to related tasks

**Challenges:**
- Model errors compound during long rollouts
- High-dimensional state spaces are difficult to model accurately

<div class="alternative-approach">

**World Models:** Learn a compressed latent representation $z_t$ and model dynamics in latent space:

$$z_{t+1} = g(z_t, a_t)$$

This reduces dimensionality and focuses on task-relevant features. Policies can be trained entirely in the latent space.

</div>

### 5.4 Safe Reinforcement Learning

Safety constraints are critical for real-world robotics deployment.

**Constrained MDP:** Formulate safety as constraints on expected cost:

$$\max_\pi J(\pi) \quad \text{subject to} \quad \mathbb{E}_\pi[C(s, a)] \leq d$$

where $C(s,a)$ is a cost function (e.g., joint torque limits, collision probability).

**Safe Exploration Strategies:**

1. **Action Shields:** Filter actions through a verified safety layer:
   $$a_{\text{safe}} = \text{SafetyFilter}(s, a_{\text{RL}})$$

2. **Probabilistic Safety:** Maintain uncertainty estimates and avoid high-uncertainty regions

3. **Curriculum Learning:** Gradually increase task difficulty as the agent improves

**Example: Safe Action Clipping**

```python
def safe_action_filter(action, state, limits):
    """
    Clip actions to satisfy safety constraints.

    Args:
        action: Proposed action (joint velocities)
        state: Current state (joint positions, velocities)
        limits: Dictionary of constraints

    Returns:
        safe_action: Filtered action satisfying constraints
    """
    joint_pos = state[:7]  # First 7 dims are positions
    joint_vel = state[7:14]  # Next 7 are velocities

    # Predicted next position
    dt = 0.01
    next_pos = joint_pos + action * dt

    # Enforce position limits
    pos_min, pos_max = limits['position']
    next_pos = np.clip(next_pos, pos_min, pos_max)

    # Recompute action to achieve clipped position
    safe_action = (next_pos - joint_pos) / dt

    # Enforce velocity limits
    vel_min, vel_max = limits['velocity']
    safe_action = np.clip(safe_action, vel_min, vel_max)

    # Enforce acceleration limits
    acc = (safe_action - joint_vel) / dt
    acc_min, acc_max = limits['acceleration']
    acc = np.clip(acc, acc_min, acc_max)
    safe_action = joint_vel + acc * dt

    return safe_action
```

### 5.5 Hierarchical Reinforcement Learning

Complex robotic tasks often exhibit hierarchical structure: high-level goals decompose into subtasks.

<div class="theory-concept">

**Options Framework:** An option $\omega = (\mathcal{I}_\omega, \pi_\omega, \beta_\omega)$ consists of:
- $\mathcal{I}_\omega \subseteq \mathcal{S}$: Initiation set (states where option can start)
- $\pi_\omega$: Intra-option policy
- $\beta_\omega: \mathcal{S} \rightarrow [0, 1]$: Termination condition

A meta-controller learns to select options; each option executes until termination.

</div>

**Advantages:**
- Temporal abstraction reduces horizon length
- Reusable skills across tasks
- Structured exploration

### Module 5 Quiz

1. **Sample Efficiency:** Why is sample efficiency particularly critical for robotics compared to game-playing domains? What techniques address this challenge?

2. **Domain Randomization:** How does training on a distribution of environments improve sim-to-real transfer? What are the limits of this approach?

3. **Model-Based RL:** Compare model-based and model-free RL for a robotic reaching task. Under what conditions would model-based methods be preferred?

4. **Safety Constraints:** Describe a scenario where an RL agent might discover an effective but unsafe policy. How would you formulate this as a constrained MDP?

---

## Summary

This course has provided a comprehensive treatment of reinforcement learning theory and its application to robotics. Key insights include:

**Theoretical Foundations:**
- Markov Decision Processes formalize sequential decision-making under uncertainty
- Value functions and Bellman equations provide the mathematical foundation for RL algorithms
- The exploration-exploitation tradeoff is fundamental to learning optimal policies
- Convergence guarantees exist for tabular methods under appropriate conditions

**Algorithmic Landscape:**
- Dynamic programming solves known MDPs exactly but requires a model
- Temporal difference learning enables model-free learning from experience
- Q-learning learns optimal action-values off-policy with convergence guarantees
- Deep Q-Networks extend value-based methods to high-dimensional observations
- Policy gradient methods directly optimize parameterized policies
- Actor-critic methods combine value functions and policy optimization

**Robotics Applications:**
- Sample efficiency is critical for real-world deployment
- Sim-to-real transfer requires domain randomization or adaptation
- Model-based RL improves data efficiency but faces modeling challenges
- Safety constraints must be incorporated for physical systems
- Hierarchical methods leverage task structure for complex behaviors

**Future Directions:**
The field continues to evolve rapidly. Promising directions include:
- Offline RL: Learning from fixed datasets without additional environment interaction
- Multi-task and meta-RL: Generalizing across task distributions
- Foundation models for robotics: Scaling RL with large datasets and architectures
- Provably safe RL: Formal verification and certified safe exploration

Reinforcement learning provides a principled framework for autonomous robots to learn complex behaviors through experience. As algorithms become more sample-efficient and safe, RL will enable robots to continuously improve and adapt to new situations throughout their operational lifetime.

---

## Optional Laboratory Exercises

The following hands-on exercises provide practical experience with reinforcement learning algorithms. These labs are optional but highly recommended for developing implementation skills.

### Lab 1: Tabular Q-Learning for CartPole

<div class="optional-exercise">

**Objective:** Implement Q-learning from scratch and solve the CartPole balancing task.

**Duration:** 90 minutes

**Tasks:**

1. **Environment Setup**
   - Install Gymnasium: `pip install gymnasium`
   - Load the CartPole-v1 environment
   - Understand the state space (4D continuous) and action space (2 discrete actions)

2. **State Discretization**
   - Implement a function to discretize the continuous state space into bins
   - Experiment with different bin sizes (e.g., 10x10x10x10 vs. 20x20x20x20)
   - Analyze the tradeoff between resolution and table size

3. **Q-Learning Implementation**
   - Implement the tabular Q-learning algorithm
   - Use ε-greedy exploration with decaying ε
   - Track episode rewards over training

4. **Analysis**
   - Plot learning curves (reward vs. episode)
   - Visualize the Q-table for selected state slices
   - Measure final policy performance (average reward over 100 episodes)

**Deliverable:** A working Q-learning implementation that achieves an average reward of ≥195 over 100 consecutive episodes, with documented analysis.

</div>

<div class="exercise-tip">

**Tip:** Start with a coarse discretization (e.g., 10 bins per dimension) to see learning progress quickly, then refine. The CartPole task doesn't require fine-grained state representation.

</div>

### Lab 2: Deep Q-Network for Continuous State Spaces

<div class="optional-exercise">

**Objective:** Implement DQN with experience replay and target networks.

**Duration:** 120 minutes

**Tasks:**

1. **Network Architecture**
   - Design a neural network to approximate Q(s, a)
   - Use 2-3 hidden layers with ReLU activations
   - Implement both online and target networks

2. **Experience Replay**
   - Implement a replay buffer to store transitions
   - Sample mini-batches uniformly for training
   - Experiment with buffer sizes (1000, 10000, 100000)

3. **Training Loop**
   - Implement the DQN training algorithm
   - Update target network every N steps
   - Use gradient clipping for stability
   - Log training metrics (loss, reward, epsilon)

4. **Hyperparameter Tuning**
   - Experiment with learning rate (1e-4, 1e-3, 1e-2)
   - Test different target update frequencies
   - Compare ε-greedy schedules (linear decay, exponential decay)

**Deliverable:** A trained DQN agent solving CartPole or Acrobot, with training curves and hyperparameter analysis.

</div>

<div class="exercise-advanced">

**Advanced Extension:** Implement Double DQN and Dueling DQN. Compare their performance to vanilla DQN on a more challenging environment like LunarLander-v2.

</div>

### Lab 3: Policy Gradient for Continuous Control

<div class="optional-exercise">

**Objective:** Implement REINFORCE with baseline for continuous action spaces.

**Duration:** 120 minutes

**Tasks:**

1. **Policy Network**
   - Implement a Gaussian policy network (outputs mean and std)
   - Ensure actions are properly bounded for the environment
   - Add entropy regularization to encourage exploration

2. **Baseline Value Function**
   - Implement a separate value network as the baseline
   - Train it to predict returns via MSE loss
   - Compute advantages for policy updates

3. **REINFORCE Algorithm**
   - Collect full episode trajectories
   - Compute returns and advantages
   - Update policy using the policy gradient theorem
   - Implement gradient clipping and learning rate scheduling

4. **Evaluation**
   - Train on Pendulum-v1 or MountainCarContinuous-v0
   - Compare performance with and without baseline
   - Analyze the effect of entropy regularization

**Deliverable:** A working REINFORCE implementation with documented performance comparisons.

</div>

<div class="exercise-tip">

**Debugging Tip:** If the policy doesn't improve, check that: (1) advantages are properly normalized, (2) gradient signs are correct (maximizing, not minimizing), (3) learning rate isn't too large.

</div>

### Lab 4: Proximal Policy Optimization (PPO)

<div class="optional-exercise">

**Objective:** Implement PPO and compare it to REINFORCE.

**Duration:** 150 minutes

**Tasks:**

1. **PPO Architecture**
   - Implement the clipped surrogate objective
   - Use shared feature extraction for actor and critic
   - Add value function clipping for stability

2. **GAE (Generalized Advantage Estimation)**
   - Implement GAE for variance-reduced advantage estimates
   - Experiment with λ parameter (0.9, 0.95, 0.99)

3. **Training Infrastructure**
   - Collect trajectories from multiple parallel environments
   - Perform multiple epochs of updates per data batch
   - Implement learning rate annealing

4. **Comparative Analysis**
   - Compare PPO to REINFORCE on sample efficiency
   - Measure training stability (reward variance)
   - Evaluate final policy performance

**Deliverable:** A complete PPO implementation with comparative analysis against REINFORCE.

</div>

### Lab 5: Robot Arm Control with RL

<div class="optional-exercise">

**Objective:** Train an RL agent to control a simulated robot arm to reach targets.

**Duration:** 180 minutes

**Tasks:**

1. **Environment Setup**
   - Use a robotic simulation environment (e.g., Pybullet, MuJoCo)
   - Define the task: reach a randomly positioned target
   - Design the reward function (negative distance to target, penalty for time)

2. **State and Action Spaces**
   - State: joint positions, velocities, target position
   - Actions: joint velocities or torques
   - Implement proper normalization for all inputs

3. **Algorithm Selection**
   - Choose an appropriate algorithm (recommended: SAC or TD3 for continuous control)
   - Implement or use a library version (e.g., Stable-Baselines3)
   - Configure hyperparameters for the robotics domain

4. **Training and Evaluation**
   - Train the agent until convergence
   - Evaluate success rate (% of episodes reaching target)
   - Measure average episode length and reward
   - Visualize trajectories in the simulator

5. **Domain Randomization**
   - Randomize target positions, initial joint angles
   - Add action noise and observation noise
   - Assess robustness to perturbations

**Deliverable:** A trained RL policy controlling a robot arm to reach targets, with ≥80% success rate and analysis of robustness.

</div>

<div class="exercise-advanced">

**Research Extension:** Implement a hierarchical RL approach where a high-level policy selects subgoals and a low-level policy executes reaching motions. Compare sample efficiency to flat RL.

</div>

### Lab 6: Sim-to-Real Transfer Challenge

<div class="optional-exercise">

**Objective:** Investigate sim-to-real transfer for RL policies.

**Duration:** 150 minutes

**Prerequisites:** Access to a physical robot or high-fidelity simulator with domain shift

**Tasks:**

1. **Baseline Training**
   - Train a policy in a simple simulation
   - Achieve strong performance (e.g., >90% success rate)
   - Evaluate on a different simulator or real robot

2. **Domain Randomization**
   - Identify key simulation parameters (friction, mass, actuator gains)
   - Implement randomization for these parameters
   - Retrain and measure sim-to-real transfer quality

3. **System Identification**
   - Collect data on the real robot (or target simulator)
   - Fit simulation parameters to match real dynamics
   - Fine-tune the policy in the identified simulator

4. **Comparative Analysis**
   - Measure the reality gap for each method
   - Quantify: success rate drop, trajectory deviation
   - Identify which aspects transfer well vs. poorly

**Deliverable:** A report on sim-to-real transfer quality with quantitative comparisons of transfer methods.

</div>

<div class="exercise-tip">

**Safety Note:** When deploying RL policies on real robots, always start with reduced action limits and implement emergency stops. RL policies can exhibit unexpected behaviors in states not seen during training.

</div>

---

## Certification Requirements

To receive certification for AI-204, students must complete:

1. **Theoretical Mastery:** Score at least 80% on all module quizzes
2. **Core Laboratory:** Successfully complete Labs 1-4, demonstrating:
   - Working implementations of Q-learning, DQN, REINFORCE, and PPO
   - Comparative analysis of algorithms
   - Documented training procedures and results
3. **Final Project:** Complete Lab 5 (Robot Arm Control), achieving:
   - ≥80% success rate on reaching task
   - Comprehensive training report with ablation studies
   - Analysis of learned behaviors

All implementations must be well-documented with clear explanations of design choices and experimental results.

---

## Further Reading

**Foundational Texts:**
- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- Bertsekas, D. P. (2019). *Reinforcement Learning and Optimal Control*. Athena Scientific.

**Seminal Papers:**
- Watkins, C. J., & Dayan, P. (1992). "Q-learning." *Machine Learning*.
- Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." *Nature*.
- Schulman, J., et al. (2017). "Proximal policy optimization algorithms." *arXiv*.

**Robotics Applications:**
- Levine, S., et al. (2016). "End-to-end training of deep visuomotor policies." *JMLR*.
- Haarnoja, T., et al. (2018). "Soft actor-critic: Off-policy maximum entropy deep RL." *ICML*.
- Andrychowicz, M., et al. (2020). "Learning dexterous in-hand manipulation." *IJRR*.

**Surveys and Tutorials:**
- Arulkumaran, K., et al. (2017). "Deep reinforcement learning: A brief survey." *IEEE Signal Processing Magazine*.
- Li, Y. (2017). "Deep reinforcement learning: An overview." *arXiv*.

**Open-Source Libraries:**
- Stable-Baselines3: https://stable-baselines3.readthedocs.io/
- RLlib: https://docs.ray.io/en/latest/rllib/
- CleanRL: https://github.com/vwxyzjn/cleanrl
