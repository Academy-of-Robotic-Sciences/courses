<!--
author:   Dr. Alex Chen
email:    alex.chen@robotcampus.dev
version:  2.0.0

language: en
narrator: US English Female

comment:  Machine Learning Fundamentals: Comprehensive treatment of supervised learning, neural network theory, gradient descent optimization, and convolutional architectures for robotics applications.

icon:     https://robotcampus.dev/logos/ai-201.png

mode:     Textbook

@style
<style>
</style>
@end

link:     https://robotcampus.dev/styles/course-styles.css

import:   https://raw.githubusercontent.com/LiaTemplates/Pyodide/master/README.md
-->

# AI-201: Machine Learning Fundamentals

> **Neural networks form the computational substrate for modern robot perception and control.**

## Course Overview

| | |
|---|---|
| **Course Code** | AI-201 |
| **Duration** | 8 hours |
| **Level** | Intermediate |
| **Prerequisites** | Python programming, linear algebra (RC-102), probability theory |
| **Theory-Practice** | 60% theory, 40% optional labs |

---

## Learning Objectives

Upon completion of this course, students will be able to:

### Theoretical Understanding

- Explain the mathematical foundations of supervised learning and function approximation
- Derive the backpropagation algorithm from first principles using the chain rule
- Analyze the universal approximation theorem and its implications for neural network expressiveness
- Understand the bias-variance tradeoff and regularization techniques
- Describe the evolution of neural network architectures from perceptrons to modern deep learning

### Practical Skills

- Implement multi-layer perceptrons using NumPy and PyTorch
- Design and train convolutional neural networks for image classification
- Apply gradient descent optimization techniques and tune hyperparameters
- Diagnose overfitting and underfitting through learning curves
- Deploy trained models for inference in robotics applications

---

## Module 1: Foundations of Machine Learning

### 1.1 The Learning Problem

<!-- class="theory-concept" -->
**Machine Learning Formalism**

Machine learning addresses the problem of learning a function $f: X \rightarrow Y$ from training data $D = \{(x_i, y_i)\}_{i=1}^{N}$, where:

- **X** is the input space (e.g., images, sensor readings)
- **Y** is the output space (e.g., class labels, continuous values)
- **N** is the number of training examples

The learning algorithm seeks to minimize the **expected risk**:

$$R(f) = \mathbb{E}_{(x,y) \sim P}[L(f(x), y)]$$

where $L$ is a loss function and $P$ is the unknown data distribution.

In practice, we minimize the **empirical risk** on training data:

$$\hat{R}(f) = \frac{1}{N} \sum_{i=1}^{N} L(f(x_i), y_i)$$

<!-- class="historical-note" -->
**Historical Development**

The formalization of statistical learning theory emerged through several key developments:

- **1950s-1960s**: Perceptron algorithm (Rosenblatt, 1957), convergence theorem (Novikoff, 1962)
- **1980s**: Backpropagation for multilayer networks (Rumelhart, Hinton, Williams, 1986)
- **1990s**: Statistical learning theory (Vapnik, 1995), support vector machines
- **2000s**: Deep learning revolution enabled by GPUs and large datasets (ImageNet, 2009)
- **2010s**: Breakthrough in image recognition (AlexNet, 2012), leading to modern deep learning era

### 1.2 Linear Models and the Perceptron

<!-- class="theory-concept" -->
**Linear Classification**

The simplest learning model is a linear classifier:

$$f(x) = \text{sign}(w^T x + b)$$

where:
- **w** ∈ ℝ^d is the weight vector
- **b** ∈ ℝ is the bias term
- **x** ∈ ℝ^d is the input feature vector

For binary classification with labels {-1, +1}, this defines a decision boundary at $w^T x + b = 0$.

**The Perceptron Learning Algorithm**:

```
Initialize w = 0, b = 0
For each training example (x, y):
    If y(w^T x + b) ≤ 0:  # Misclassified
        w ← w + yx
        b ← b + y
```

The perceptron converges to a separating hyperplane if the data is linearly separable.

<!-- class="theory-concept" -->
**Limitations of Linear Models**

Linear models cannot solve problems that are not linearly separable. The classic example is the XOR problem:

| x₁ | x₂ | XOR |
|----|----| ----|
| 0  | 0  |  0  |
| 0  | 1  |  1  |
| 1  | 0  |  1  |
| 1  | 1  |  0  |

No linear decision boundary can correctly classify all XOR examples. This limitation motivated the development of multilayer networks.

### 1.3 From Linear to Nonlinear: Activation Functions

<!-- class="theory-concept" -->
**Introducing Nonlinearity**

A single neuron with a nonlinear activation function φ computes:

$$y = \phi(w^T x + b)$$

Common activation functions include:

1. **Sigmoid**: $\sigma(z) = \frac{1}{1 + e^{-z}}$
   - Output range: (0, 1)
   - Historically popular but suffers from vanishing gradients
   - Derivative: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

2. **Hyperbolic Tangent**: $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
   - Output range: (-1, 1)
   - Zero-centered, but still has vanishing gradient problem
   - Derivative: $\tanh'(z) = 1 - \tanh^2(z)$

3. **ReLU (Rectified Linear Unit)**: $\text{ReLU}(z) = \max(0, z)$
   - Output range: [0, ∞)
   - Most popular in modern deep learning
   - Derivative: 1 if z > 0, else 0
   - Advantages: Computationally efficient, mitigates vanishing gradients

4. **Leaky ReLU**: $\text{LeakyReLU}(z) = \max(\alpha z, z)$ where α ≈ 0.01
   - Addresses "dying ReLU" problem
   - Allows small negative values

**Quiz: Understanding Activations**

What is the output of σ(0)?

[( )] 0
[(X)] 0.5
[( )] 1
[( )] Undefined

Why is ReLU preferred over sigmoid in hidden layers of deep networks?

[( )] ReLU is bounded between 0 and 1
[(X)] ReLU mitigates vanishing gradient problems
[( )] ReLU is differentiable everywhere
[( )] ReLU produces negative outputs

### 1.4 Multi-Layer Perceptrons (MLPs)

<!-- class="theory-concept" -->
**Network Architecture**

A multi-layer perceptron consists of:

- **Input layer**: Receives feature vector x ∈ ℝ^d
- **Hidden layers**: One or more layers of neurons with nonlinear activations
- **Output layer**: Produces predictions ŷ

For a network with one hidden layer of h neurons:

$$z^{(1)} = W^{(1)} x + b^{(1)}$$
$$a^{(1)} = \phi(z^{(1)})$$
$$z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$$
$$\hat{y} = \phi_{\text{out}}(z^{(2)})$$

where:
- $W^{(1)} \in \mathbb{R}^{h \times d}$ are weights from input to hidden layer
- $W^{(2)} \in \mathbb{R}^{k \times h}$ are weights from hidden to output layer
- φ is the hidden layer activation (typically ReLU)
- φ_out is the output activation (depends on task)

<!-- class="theory-concept" -->
**Universal Approximation Theorem**

**Theorem** (Hornik, Stinchcombe, White, 1989): A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of ℝ^d to arbitrary accuracy, given appropriate nonlinear activation functions.

**Implications**:
- Neural networks are extremely expressive function approximators
- Depth (multiple layers) is not theoretically necessary for representation
- However, depth can dramatically improve efficiency and generalization
- Modern deep networks use many layers despite the theoretical sufficiency of one

---

## Module 2: Training Neural Networks

### 2.1 Loss Functions

<!-- class="theory-concept" -->
**Measuring Prediction Quality**

The loss function L(ŷ, y) quantifies the discrepancy between predictions ŷ and true labels y.

**For Regression (continuous outputs)**:

**Mean Squared Error (MSE)**:
$$L_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

**Mean Absolute Error (MAE)**:
$$L_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

**Huber Loss** (robust to outliers):
$$L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}$$

**For Classification (discrete outputs)**:

**Binary Cross-Entropy** (for binary classification):
$$L_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$

**Categorical Cross-Entropy** (for multi-class classification):
$$L_{\text{CCE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})$$

where C is the number of classes and $y_{i,c}$ is 1 if example i belongs to class c, else 0.

**Quiz: Loss Functions**

Which loss function is most appropriate for a robot vision system classifying objects into 10 different categories?

[( )] Mean Squared Error
[( )] Binary Cross-Entropy
[(X)] Categorical Cross-Entropy
[( )] Huber Loss

### 2.2 Gradient Descent Optimization

<!-- class="theory-concept" -->
**The Optimization Problem**

Training a neural network requires finding parameters θ (all weights and biases) that minimize the loss:

$$\theta^* = \arg\min_{\theta} L(\theta) = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^{N} L(f_{\theta}(x_i), y_i)$$

**Gradient Descent** iteratively updates parameters in the direction of steepest descent:

$$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta_t)$$

where η is the **learning rate**.

**Variants of Gradient Descent**:

1. **Batch Gradient Descent**: Compute gradient over entire dataset
   - Exact gradient computation
   - Computationally expensive for large datasets
   - Deterministic updates

2. **Stochastic Gradient Descent (SGD)**: Compute gradient on single example
   - Fast per-iteration updates
   - High variance, noisy gradients
   - Can escape shallow local minima

3. **Mini-Batch Gradient Descent**: Compute gradient on small batch (typically 32-256 examples)
   - Balance between accuracy and efficiency
   - Enables GPU parallelization
   - Most common in practice

<!-- class="theory-concept" -->
**Advanced Optimizers**

**Momentum** (Polyak, 1964):
```
v_t = β v_{t-1} + ∇L(θ_t)
θ_{t+1} = θ_t - η v_t
```
- Accumulates velocity in consistent gradient directions
- Dampens oscillations
- Typical β ≈ 0.9

**Adam** (Adaptive Moment Estimation, Kingma & Ba, 2014):
```
m_t = β₁ m_{t-1} + (1 - β₁) ∇L(θ_t)    # First moment estimate
v_t = β₂ v_{t-1} + (1 - β₂) (∇L(θ_t))²  # Second moment estimate
m̂_t = m_t / (1 - β₁^t)                  # Bias correction
v̂_t = v_t / (1 - β₂^t)
θ_{t+1} = θ_t - η m̂_t / (√v̂_t + ε)
```
- Adaptive learning rates per parameter
- Combines benefits of momentum and RMSprop
- Default choice for most applications
- Typical values: β₁ = 0.9, β₂ = 0.999, ε = 10⁻⁸

### 2.3 Backpropagation Algorithm

<!-- class="theory-concept" -->
**Computing Gradients Efficiently**

Backpropagation computes gradients of the loss with respect to all parameters using the chain rule of calculus.

**Forward Pass**: Compute activations layer by layer:
```
z^(1) = W^(1) x + b^(1)
a^(1) = φ(z^(1))
z^(2) = W^(2) a^(1) + b^(2)
ŷ = φ_out(z^(2))
L = loss(ŷ, y)
```

**Backward Pass**: Compute gradients layer by layer (reverse order):

For output layer:
$$\delta^{(2)} = \frac{\partial L}{\partial z^{(2)}} = \frac{\partial L}{\partial \hat{y}} \odot \phi_{\text{out}}'(z^{(2)})$$

For hidden layer (chain rule):
$$\delta^{(1)} = \frac{\partial L}{\partial z^{(1)}} = (W^{(2)T} \delta^{(2)}) \odot \phi'(z^{(1)})$$

Parameter gradients:
$$\frac{\partial L}{\partial W^{(2)}} = \delta^{(2)} (a^{(1)})^T$$
$$\frac{\partial L}{\partial b^{(2)}} = \delta^{(2)}$$
$$\frac{\partial L}{\partial W^{(1)}} = \delta^{(1)} x^T$$
$$\frac{\partial L}{\partial b^{(1)}} = \delta^{(1)}$$

where ⊙ denotes element-wise multiplication.

**Computational Complexity**: Backpropagation has the same computational cost as the forward pass, making gradient computation efficient.

**Quiz: Backpropagation**

The backpropagation algorithm computes gradients by:

[( )] Numerically approximating derivatives
[(X)] Applying the chain rule in reverse order through the network
[( )] Randomly sampling gradient directions
[( )] Solving a linear system

### 2.4 Regularization and Generalization

<!-- class="theory-concept" -->
**The Bias-Variance Tradeoff**

Machine learning models face a fundamental tradeoff:

- **Bias**: Error from incorrect assumptions (underfitting)
- **Variance**: Error from sensitivity to training data fluctuations (overfitting)

**Total Error** = Bias² + Variance + Irreducible Error

**Overfitting**: Model performs well on training data but poorly on unseen test data. Caused by:
- Model too complex relative to data
- Insufficient training data
- Training for too many epochs

**Regularization Techniques**:

1. **L2 Regularization (Weight Decay)**:
   $$L_{\text{reg}} = L_{\text{data}} + \lambda \sum_{i} w_i^2$$
   - Penalizes large weights
   - Encourages simpler models
   - λ controls regularization strength

2. **L1 Regularization (Lasso)**:
   $$L_{\text{reg}} = L_{\text{data}} + \lambda \sum_{i} |w_i|$$
   - Encourages sparse weights (many exact zeros)
   - Performs implicit feature selection

3. **Dropout** (Srivastava et al., 2014):
   - Randomly "drop" (set to zero) neurons during training with probability p
   - Forces network to learn redundant representations
   - Acts as ensemble of sub-networks
   - Typical p ≈ 0.5 for hidden layers, 0.2 for input

4. **Early Stopping**:
   - Monitor validation loss during training
   - Stop when validation loss stops improving
   - Simple and effective

5. **Data Augmentation**:
   - Artificially increase training data through transformations
   - For images: rotations, flips, crops, color jitter
   - Particularly effective for computer vision

**Cross-Validation**: Split data into training, validation, and test sets:
- **Training**: Used to update parameters
- **Validation**: Used to tune hyperparameters and monitor overfitting
- **Test**: Final evaluation, never used during development

---

## Module 3: Convolutional Neural Networks

### 3.1 Motivation: Visual Perception

<!-- class="theory-concept" -->
**The Challenge of Image Classification**

Consider classifying a 224×224 RGB image into 1000 categories:

- Input dimension: 224 × 224 × 3 = 150,528 pixels
- Fully-connected first layer with 1000 hidden units: 150,528,000 parameters
- This is computationally prohibitive and prone to overfitting

**Key Insights from Neuroscience**:

1. **Local Receptive Fields**: Visual neurons respond to stimuli in restricted spatial regions
2. **Hierarchy of Features**: Simple cells detect edges, complex cells detect patterns, higher areas detect objects
3. **Translation Invariance**: Object recognition is largely independent of position

Convolutional Neural Networks (CNNs) incorporate these principles through specialized architecture.

### 3.2 Convolutional Layers

<!-- class="theory-concept" -->
**The Convolution Operation**

A convolutional layer applies learned filters (kernels) to the input:

$$(I * K)_{i,j} = \sum_{m} \sum_{n} I_{i+m, j+n} \cdot K_{m,n}$$

where:
- **I** is the input image or feature map
- **K** is the filter/kernel (typically 3×3, 5×5, or 7×7)
- * denotes convolution

**Properties of Convolution**:

1. **Parameter Sharing**: Same filter applied across entire image
   - Drastically reduces parameters
   - Learns translation-invariant features

2. **Local Connectivity**: Each output depends on small spatial region
   - Exploits spatial locality in images
   - Matches biological receptive fields

3. **Hierarchical Feature Learning**:
   - Early layers: edges, gradients, colors
   - Middle layers: textures, patterns
   - Deep layers: object parts, semantic concepts

**Example**: A 3×3 edge detection filter:
```
K_vertical = [[-1  0  1]
              [-1  0  1]
              [-1  0  1]]
```

Applying this filter to an image produces large responses at vertical edges.

### 3.3 Pooling Layers

<!-- class="theory-concept" -->
**Spatial Down-Sampling**

Pooling layers reduce spatial dimensions while retaining important information:

**Max Pooling** (most common):
- Divide input into non-overlapping regions
- Take maximum value in each region
- Typical size: 2×2 with stride 2 (reduces dimensions by half)

**Average Pooling**:
- Take average value in each region
- Smoother but less commonly used

**Benefits of Pooling**:
1. Reduces computational cost
2. Provides translation invariance
3. Increases receptive field of subsequent layers
4. Reduces overfitting

### 3.4 CNN Architectures

<!-- class="theory-concept" -->
**LeNet-5** (LeCun et al., 1998)

The pioneering CNN architecture for digit recognition:

```
Input (32×32)
→ Conv (6 filters, 5×5)
→ Pool (2×2)
→ Conv (16 filters, 5×5)
→ Pool (2×2)
→ FC (120)
→ FC (84)
→ Output (10)
```

**AlexNet** (Krizhevsky, Sutskever, Hinton, 2012)

Breakthrough architecture that won ImageNet 2012:

- 5 convolutional layers
- 3 fully-connected layers
- ReLU activations
- Dropout regularization
- Data augmentation
- 60 million parameters

**Modern Trends**:
- **Deeper networks**: ResNet (up to 152 layers), VGG (16-19 layers)
- **Skip connections**: Enable training very deep networks (ResNet, DenseNet)
- **Efficient architectures**: MobileNet, EfficientNet for embedded systems
- **Transformer architectures**: Vision Transformers (ViT) challenge CNN dominance

**Quiz: CNN Architecture**

What is the primary advantage of convolutional layers over fully-connected layers for image processing?

[( )] Convolutional layers are faster to train
[(X)] Convolutional layers have fewer parameters due to parameter sharing
[( )] Convolutional layers always achieve higher accuracy
[( )] Convolutional layers require less memory

Max pooling with 2×2 windows and stride 2 applied to a 64×64 image produces output of size:

[( )] 64×64
[( )] 63×63
[(X)] 32×32
[( )] 16×16

---

## Module 4: PyTorch Implementation

### 4.1 Tensor Operations

<!-- class="theory-concept" -->
**Tensors: The Fundamental Data Structure**

PyTorch represents all data as tensors (multi-dimensional arrays):

```python
import torch

# Creating tensors
x = torch.tensor([1.0, 2.0, 3.0])              # 1D tensor (vector)
A = torch.tensor([[1, 2], [3, 4]])             # 2D tensor (matrix)
B = torch.randn(3, 224, 224)                   # 3D tensor (RGB image)
batch = torch.randn(32, 3, 224, 224)           # 4D tensor (batch of images)

# Operations
y = x * 2 + 1                                   # Element-wise operations
C = A @ A.T                                     # Matrix multiplication
norm = torch.norm(x)                            # L2 norm

# Moving to GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x_gpu = x.to(device)
```

### 4.2 Automatic Differentiation

<!-- class="theory-concept" -->
**Autograd: Automatic Gradient Computation**

PyTorch automatically computes gradients through computational graphs:

```python
# Enable gradient tracking
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# Forward pass
y = (x ** 2).sum()

# Backward pass (compute gradients)
y.backward()

# Access gradients
print(x.grad)  # tensor([2., 4., 6.])

# Gradients accumulate, so zero them between iterations
x.grad.zero_()
```

**Computational Graph**: PyTorch builds a dynamic graph tracking all operations, enabling automatic differentiation via chain rule.

### 4.3 Building a Neural Network

<!-- class="theory-concept" -->
**Defining Models with nn.Module**

```python
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    """Multi-layer perceptron for classification."""

    def __init__(self, input_dim, hidden_dim, num_classes):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        # Flatten image if needed
        x = x.view(x.size(0), -1)

        # Hidden layers with ReLU and dropout
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)

        # Output layer (logits)
        x = self.fc3(x)
        return x

# Instantiate model
model = MLP(input_dim=784, hidden_dim=256, num_classes=10)
```

**CNN Example**:

```python
class SimpleCNN(nn.Module):
    """Simple CNN for image classification."""

    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        # Fully-connected layers
        self.fc1 = nn.Linear(64 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Conv -> ReLU -> Pool
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))

        # Flatten
        x = x.view(-1, 64 * 8 * 8)

        # Fully-connected
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

### 4.4 Training Loop

<!-- class="theory-concept" -->
**Complete Training Pipeline**

```python
import torch.optim as optim
from torch.utils.data import DataLoader

# Setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleCNN(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # Set to training mode
    running_loss = 0.0

    for batch_idx, (images, labels) in enumerate(train_loader):
        # Move data to device
        images, labels = images.to(device), labels.to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()

        # Update parameters
        optimizer.step()

        running_loss += loss.item()

    # Validation
    model.eval()  # Set to evaluation mode
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # Disable gradient computation
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            # Compute accuracy
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Epoch {epoch+1}/{num_epochs}')
    print(f'  Train Loss: {running_loss/len(train_loader):.4f}')
    print(f'  Val Loss: {val_loss/len(val_loader):.4f}')
    print(f'  Val Acc: {100 * correct / total:.2f}%')
```

---

## Module 5: Practical Considerations

### 5.1 Hyperparameter Tuning

<!-- class="theory-concept" -->
**Critical Hyperparameters**

**Learning Rate** (most important):
- Too high: Training diverges
- Too low: Training too slow, may get stuck
- Typical range: 10⁻⁴ to 10⁻²
- Use learning rate schedules (decay over time)

**Batch Size**:
- Larger batches: More stable gradients, better GPU utilization
- Smaller batches: More noise, can help generalization
- Typical range: 32-256

**Number of Layers and Units**:
- Start simple, add complexity if needed
- Monitor validation performance

**Regularization Strength**:
- Weight decay λ: typically 10⁻⁵ to 10⁻³
- Dropout p: typically 0.2-0.5

**Optimization Strategy**:
- Grid search: Exhaustive but expensive
- Random search: Often more efficient
- Bayesian optimization: Principled but complex

### 5.2 Diagnosing Training Problems

<!-- class="theory-concept" -->
**Common Issues and Solutions**

**Loss not decreasing**:
- Check data loading and labels
- Reduce learning rate
- Simplify model architecture
- Check for bugs in forward/backward pass

**Overfitting** (train accuracy >> val accuracy):
- Add regularization (dropout, weight decay)
- Get more training data
- Use data augmentation
- Reduce model complexity
- Early stopping

**Underfitting** (both train and val accuracy low):
- Increase model capacity
- Train longer
- Reduce regularization
- Check for bugs

**Vanishing/Exploding Gradients**:
- Use ReLU instead of sigmoid/tanh
- Add batch normalization
- Use gradient clipping
- Use skip connections (ResNet)

<!-- class="alternative-approach" -->
**Batch Normalization**

Batch normalization (Ioffe & Szegedy, 2015) normalizes layer inputs across a mini-batch:

$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

Benefits:
- Reduces internal covariate shift
- Allows higher learning rates
- Provides regularization effect
- Widely used in modern architectures

```python
self.bn1 = nn.BatchNorm2d(64)
x = self.bn1(F.relu(self.conv1(x)))
```

### 5.3 Model Deployment for Robotics

<!-- class="theory-concept" -->
**From Training to Inference**

**Exporting Models**:

```python
# Save entire model
torch.save(model, 'model.pth')

# Save only state dict (preferred)
torch.save(model.state_dict(), 'model_weights.pth')

# Load for inference
model = SimpleCNN(num_classes=10)
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()
```

**Inference Pipeline**:

```python
import cv2
from torchvision import transforms

# Preprocessing pipeline
preprocess = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])

def classify_image(image_path, model):
    """Classify a single image."""
    # Load image
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Preprocess
    input_tensor = preprocess(img).unsqueeze(0)

    # Inference
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = F.softmax(output, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()

    return predicted_class, probabilities
```

**Real-Time Considerations**:
- **Latency**: Inference time must meet control loop requirements
- **Model size**: Embedded systems have memory constraints
- **Quantization**: Reduce precision (FP32 → INT8) for faster inference
- **Model pruning**: Remove unnecessary parameters
- **Hardware acceleration**: Use GPUs, TPUs, or specialized AI chips

---

## Summary

This course established the mathematical and computational foundations of machine learning for robotics:

1. **Supervised Learning Theory**: Formalized learning as function approximation from data
2. **Neural Networks**: Studied architecture, universal approximation, and expressiveness
3. **Optimization**: Examined gradient descent, backpropagation, and advanced optimizers
4. **Convolutional Networks**: Understood hierarchical feature learning for visual perception
5. **PyTorch Implementation**: Gained practical skills in implementing and training networks

**Key Takeaways**:
- Neural networks are universal function approximators learned through gradient descent
- Convolutional architectures exploit spatial structure for efficient visual perception
- Regularization and careful tuning are essential for generalization
- PyTorch provides flexible, efficient tools for implementing learning systems

**Next Steps**: AI-202 applies these foundations to computer vision for robotics, studying object detection, tracking, and real-time perception pipelines.

---

## Optional Laboratory Exercises

### Lab 1: Multi-Layer Perceptron from Scratch (2 hours)

<!-- class="optional-exercise" -->
**Objective**: Implement and train a multi-layer perceptron using only NumPy to solidify understanding of forward/backward propagation.

**Tasks**:
1. Implement forward pass for 2-layer network
2. Implement backward pass using chain rule
3. Train on MNIST handwritten digit dataset
4. Achieve >90% test accuracy

<!-- class="exercise-tip" -->
**Tips**:
- Start with small network (e.g., 784-128-10)
- Use ReLU activation for hidden layer
- Implement mini-batch gradient descent
- Verify gradients with numerical differentiation

### Lab 2: Image Classifier with PyTorch (3 hours)

<!-- class="optional-exercise" -->
**Objective**: Build, train, and evaluate a CNN image classifier for robot object recognition.

**Dataset**: CIFAR-10 (10 object classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)

**Tasks**:
1. Implement CNN architecture with 3-4 convolutional layers
2. Set up training pipeline with data augmentation
3. Train model and monitor learning curves
4. Evaluate on test set and analyze failure cases
5. Export model for deployment

**Target Performance**: >75% test accuracy

<!-- class="exercise-advanced" -->
**Advanced Extensions**:
- Implement learning rate scheduling
- Compare different architectures (vary depth/width)
- Try transfer learning from pre-trained ResNet
- Visualize learned filters and feature maps

### Lab 3: Overfitting Analysis (1 hour)

<!-- class="optional-exercise" -->
**Objective**: Systematically study overfitting and regularization techniques.

**Tasks**:
1. Intentionally create overfitting by training large model on small dataset
2. Plot training vs. validation loss over epochs
3. Apply different regularization techniques:
   - L2 weight decay
   - Dropout
   - Data augmentation
   - Early stopping
4. Compare validation performance across methods

<!-- class="exercise-tip" -->
**Analysis Questions**:
- At what epoch does overfitting begin?
- Which regularization method is most effective?
- How does regularization strength affect the bias-variance tradeoff?

---

*Version 2.0.0 - Academic curriculum emphasizing theoretical foundations with optional practical exercises*
