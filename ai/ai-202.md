<!--
author:   Dr. Sarah Martinez
email:    sarah.martinez@academy-of-robotic-sciences.github.io
version:  2.0.0

language: en
narrator: US English Female

comment:  Computer Vision for Robotics: Comprehensive treatment of classical and modern vision algorithms, object detection, tracking, and real-time perception pipelines for autonomous systems.


mode:     Textbook

@style
<style>
</style>
@end

link:     https://raw.githubusercontent.com/Academy-of-Robotic-Sciences/courses/main/course-styles.css

import:   https://raw.githubusercontent.com/LiaTemplates/Pyodide/master/README.md
-->

# AI-202: Computer Vision for Robotics

> **Machine perception transforms photons into actionable understanding of the physical world.**

## Course Overview

| | |
|---|---|
| **Course Code** | AI-202 |
| **Duration** | 6 hours |
| **Level** | Intermediate |
| **Prerequisites** | AI-201 (Machine Learning Fundamentals), linear algebra, Python |
| **Theory-Practice** | 60% theory, 40% optional labs |

---

## Learning Objectives

Upon completion of this course, students will be able to:

### Theoretical Understanding

- Explain the mathematical foundations of image processing and feature detection
- Compare classical computer vision approaches with deep learning-based methods
- Analyze object detection architectures and their computational trade-offs
- Understand visual tracking algorithms and temporal consistency
- Describe the evolution from hand-crafted features to learned representations

### Practical Skills

- Implement image processing pipelines using OpenCV
- Deploy pre-trained object detectors for real-time inference
- Build multi-object tracking systems with temporal filtering
- Estimate 3D object locations from 2D detections
- Integrate vision systems with ROS2 for robot control

---

## Module 1: Image Processing Fundamentals

### 1.1 Digital Images and Color Spaces

<!-- class="theory-concept" -->
**Image Representation**

A digital image is a discrete 2D function I(x, y) mapping spatial coordinates to intensity values:

- **Grayscale image**: I: ℤ² → [0, 255] (single channel)
- **RGB color image**: I: ℤ² → [0, 255]³ (three channels)
- **Resolution**: Width W × Height H pixels

**Color Spaces**:

1. **RGB (Red, Green, Blue)**:
   - Additive color model
   - Hardware-native representation (displays, cameras)
   - Channels: R, G, B ∈ [0, 255]

2. **HSV (Hue, Saturation, Value)**:
   - Cylindrical color space
   - Separates color (H) from intensity (V)
   - Useful for color-based segmentation
   - H ∈ [0, 360°], S, V ∈ [0, 1]

3. **Grayscale Conversion**:
   $$I_{\text{gray}} = 0.299R + 0.587G + 0.114B$$
   (Weighted by human luminance perception)

```python
import cv2
import numpy as np

# Read image
img = cv2.imread('image.jpg')  # BGR format (OpenCV convention)

# Convert color spaces
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
```

<!-- class="historical-note" -->
**Historical Development**

Digital image processing emerged as a distinct field in the 1960s:

- **1960s**: NASA develops image processing for space exploration (lunar mapping)
- **1970s**: Medical imaging (CT scans), first digital cameras
- **1980s**: Marr's computational vision theory, stereo vision algorithms
- **1990s**: SIFT features (1999), real-time vision becomes feasible
- **2000s**: Viola-Jones face detector (2001), bag-of-visual-words
- **2010s**: Deep learning revolution (AlexNet 2012, R-CNN 2014, YOLO 2016)

### 1.2 Image Filtering and Convolution

<!-- class="theory-concept" -->
**The Convolution Operation**

Image filtering applies a kernel K to an image I through discrete convolution:

$$(I * K)[x, y] = \sum_{i=-k}^{k} \sum_{j=-k}^{k} I[x+i, y+j] \cdot K[i, j]$$

where K is a (2k+1) × (2k+1) kernel.

**Common Filters**:

1. **Box Filter** (Averaging):
   $$K_{\text{box}} = \frac{1}{9} \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$$
   - Smooths image, reduces noise
   - Loses edge information

2. **Gaussian Filter**:
   $$G(x, y) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)$$
   - Weighted averaging with Gaussian weights
   - Preserves more structure than box filter
   - σ controls blur amount

3. **Sobel Filter** (Edge Detection):
   $$G_x = \begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}, \quad G_y = \begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}$$

   Gradient magnitude: $||\nabla I|| = \sqrt{G_x^2 + G_y^2}$

   Gradient direction: $\theta = \arctan(G_y / G_x)$

```python
# Gaussian blur
blurred = cv2.GaussianBlur(img, (5, 5), sigmaX=1.0)

# Sobel edge detection
sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
```

### 1.3 Morphological Operations

<!-- class="theory-concept" -->
**Binary Image Processing**

Morphological operations manipulate image structure using structuring elements:

**Erosion** (shrinks objects):
$$(I \ominus S)[x, y] = \min_{(i,j) \in S} I[x+i, y+j]$$

**Dilation** (expands objects):
$$(I \oplus S)[x, y] = \max_{(i,j) \in S} I[x+i, y+j]$$

**Opening** (erosion followed by dilation):
- Removes small objects and noise
- Smooths object boundaries

**Closing** (dilation followed by erosion):
- Fills small holes
- Connects nearby objects

```python
kernel = np.ones((5, 5), np.uint8)

# Morphological operations
eroded = cv2.erode(binary_img, kernel, iterations=1)
dilated = cv2.dilate(binary_img, kernel, iterations=1)
opened = cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, kernel)
closed = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)
```

### 1.4 Color-Based Segmentation

<!-- class="theory-concept" -->
**Thresholding and Masking**

Color segmentation isolates objects based on color properties:

**Binary Thresholding**:
$$I_{\text{binary}}[x, y] = \begin{cases}
255 & \text{if } I[x, y] > T \\
0 & \text{otherwise}
\end{cases}$$

**HSV Color Thresholding** (robust to lighting changes):

```python
# Define color range in HSV
lower_red = np.array([0, 100, 100])
upper_red = np.array([10, 255, 255])

# Create mask
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv, lower_red, upper_red)

# Apply mask
result = cv2.bitwise_and(img, img, mask=mask)
```

**Contour Detection**:

```python
contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL,
                                        cv2.CHAIN_APPROX_SIMPLE)

for contour in contours:
    area = cv2.contourArea(contour)
    if area > 500:  # Filter small contours
        x, y, w, h = cv2.boundingRect(contour)
        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
```

**Quiz: Image Processing**

What is the primary advantage of using HSV color space for object segmentation compared to RGB?

[( )] HSV is faster to compute
[(X)] HSV separates color from intensity, making it more robust to lighting changes
[( )] HSV has fewer channels
[( )] HSV is the native camera format

---

## Module 2: Classical Feature Detection

### 2.1 Corner Detection

<!-- class="theory-concept" -->
**What is a Corner?**

A corner is a point where image gradient changes significantly in multiple directions:
- Edges: Strong gradient in one direction
- Corners: Strong gradients in two orthogonal directions
- Flat regions: No significant gradient

**Harris Corner Detector** (Harris & Stephens, 1988):

For each pixel, compute structure tensor (second moment matrix):

$$M = \sum_{(x,y) \in W} \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}$$

where $I_x, I_y$ are image gradients and W is a local window.

Corner response:
$$R = \det(M) - k \cdot \text{trace}(M)^2 = \lambda_1 \lambda_2 - k(\lambda_1 + \lambda_2)^2$$

where λ₁, λ₂ are eigenvalues of M, and k ≈ 0.04-0.06.

**Interpretation**:
- Both λ₁, λ₂ large → corner (R is large and positive)
- One eigenvalue large → edge (R is small or negative)
- Both eigenvalues small → flat region (R ≈ 0)

```python
# Harris corner detection
gray_float = np.float32(gray)
corners = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)

# Threshold for corner detection
img[corners > 0.01 * corners.max()] = [0, 0, 255]
```

<!-- class="alternative-approach" -->
**FAST (Features from Accelerated Segment Test)**

FAST (Rosten & Drummond, 2006) is an efficient corner detector:

Algorithm:
1. For each pixel p with intensity Ip
2. Consider circle of 16 pixels around p
3. p is a corner if there exists a set of N contiguous pixels in the circle
   that are all brighter or all darker than Ip + t

Advantages:
- Extremely fast (real-time on embedded systems)
- Good repeatability
- Widely used in SLAM systems

```python
fast = cv2.FastFeatureDetector_create()
keypoints = fast.detect(gray, None)
img_with_keypoints = cv2.drawKeypoints(img, keypoints, None,
                                        color=(255, 0, 0))
```

### 2.2 Scale-Invariant Features

<!-- class="theory-concept" -->
**SIFT: Scale-Invariant Feature Transform** (Lowe, 1999)

SIFT detects and describes features that are invariant to:
- Scale (image zoom)
- Rotation
- Moderate affine transformations
- Illumination changes

**SIFT Algorithm**:

1. **Scale-Space Extrema Detection**:
   - Build Gaussian pyramid and Difference-of-Gaussian (DoG)
   - Detect local maxima/minima across scales

2. **Keypoint Localization**:
   - Refine keypoint positions
   - Reject low-contrast and edge responses

3. **Orientation Assignment**:
   - Compute gradient histogram in keypoint region
   - Assign dominant orientation(s)

4. **Descriptor Construction**:
   - 16×16 region around keypoint
   - Divided into 4×4 subregions
   - 8-bin orientation histogram per subregion
   - Final descriptor: 128-dimensional vector

```python
sift = cv2.SIFT_create()
keypoints, descriptors = sift.detectAndCompute(gray, None)

# Match features between two images
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
matches = bf.match(descriptors1, descriptors2)
matches = sorted(matches, key=lambda x: x.distance)
```

<!-- class="alternative-approach" -->
**ORB: Oriented FAST and Rotated BRIEF**

ORB (Rublee et al., 2011) is a fast, free alternative to SIFT:

- **Detector**: Oriented FAST (adds orientation to FAST corners)
- **Descriptor**: Rotated BRIEF (binary descriptor)
- **Advantages**: Very fast, rotation-invariant, free to use
- **Trade-off**: Less robust than SIFT to scale/affine changes

```python
orb = cv2.ORB_create()
keypoints, descriptors = orb.detectAndCompute(gray, None)

# ORB descriptors are binary (use Hamming distance)
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(descriptors1, descriptors2)
```

**Quiz: Feature Detection**

Which feature detector is best suited for real-time SLAM on a resource-constrained mobile robot?

[( )] SIFT (highest quality features)
[(X)] ORB (fast and efficient)
[( )] Harris corners (simplest algorithm)
[( )] Manual feature selection

---

## Module 3: Deep Learning for Object Detection

### 3.1 The Object Detection Problem

<!-- class="theory-concept" -->
**Task Definition**

Object detection requires:
1. **Localization**: Where is the object? (bounding box: x, y, w, h)
2. **Classification**: What is the object? (class label)

For an image with N objects, output:
$$\{(x_i, y_i, w_i, h_i, c_i, p_i)\}_{i=1}^{N}$$

where:
- (x_i, y_i): Top-left corner
- (w_i, h_i): Width and height
- c_i: Class label
- p_i: Confidence score

**Evaluation Metrics**:

**Intersection over Union (IoU)**:
$$\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}$$

**Average Precision (AP)**:
- Compute precision-recall curve
- AP is area under curve
- mAP (mean AP) averages over all classes

### 3.2 Evolution of Object Detectors

<!-- class="historical-note" -->
**R-CNN Family** (Region-based CNNs)

**R-CNN** (Girshick et al., 2014):
1. Generate ~2000 region proposals (selective search)
2. Warp each region to fixed size
3. Extract features with CNN
4. Classify with SVM

Limitations: Slow (2000 CNN forward passes per image)

**Fast R-CNN** (Girshick, 2015):
1. Single CNN forward pass for entire image
2. ROI pooling to extract fixed-size features
3. Classification and bounding box regression

**Faster R-CNN** (Ren et al., 2016):
- Replace selective search with Region Proposal Network (RPN)
- End-to-end trainable
- ~7 FPS

### 3.3 YOLO: You Only Look Once

<!-- class="theory-concept" -->
**Single-Shot Detection** (Redmon et al., 2016)

YOLO reframes object detection as a single regression problem:

**Architecture**:
1. Divide image into S×S grid
2. Each grid cell predicts B bounding boxes
3. Each box predicts: (x, y, w, h, confidence, class probabilities)

**Loss Function** (multi-part):
$$L = \lambda_{\text{coord}} \sum \text{(localization loss)} + \sum \text{(confidence loss)} + \sum \text{(classification loss)}$$

**Advantages**:
- Very fast (45-155 FPS depending on variant)
- Reasons globally about image (fewer background false positives)
- Learns generalizable representations

**YOLOv5 Example** (Ultralytics):

```python
import torch

# Load pre-trained model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Inference
results = model(img)  # img can be PIL, np.array, or path

# Results
results.print()  # Print detections to console
results.show()   # Display image with boxes
results.save()   # Save results

# Access detections
detections = results.xyxy[0]  # [x1, y1, x2, y2, conf, class]
for *box, conf, cls in detections:
    x1, y1, x2, y2 = box
    class_name = model.names[int(cls)]
    print(f"{class_name}: {conf:.2f} at ({x1}, {y1}, {x2}, {y2})")
```

<!-- class="alternative-approach" -->
**Other Modern Detectors**

**SSD (Single Shot Multibox Detector)**:
- Multi-scale feature maps
- Default boxes (similar to anchors)
- Fast and accurate

**RetinaNet**:
- Feature Pyramid Network (FPN)
- Focal loss for class imbalance
- State-of-the-art accuracy

**EfficientDet**:
- Compound scaling of backbone, FPN, box/class network
- Best accuracy/efficiency trade-off

**Transformer-based** (DETR, 2020):
- No anchors, no NMS (non-maximum suppression)
- Direct set prediction
- End-to-end differentiable

### 3.4 Real-Time Detection Pipeline

<!-- class="theory-concept" -->
**Practical Implementation**

```python
import cv2
import torch

class ObjectDetector:
    """Real-time object detector using YOLOv5."""

    def __init__(self, model_name='yolov5s', conf_threshold=0.5):
        self.model = torch.hub.load('ultralytics/yolov5', model_name)
        self.model.conf = conf_threshold

    def detect(self, frame):
        """
        Detect objects in a single frame.

        Returns:
            detections: List of (class, confidence, x1, y1, x2, y2)
        """
        results = self.model(frame)
        detections = []

        for *box, conf, cls in results.xyxy[0]:
            class_name = self.model.names[int(cls)]
            detections.append((class_name, float(conf),
                             *[int(x) for x in box]))

        return detections

    def draw_boxes(self, frame, detections):
        """Draw bounding boxes on frame."""
        for class_name, conf, x1, y1, x2, y2 in detections:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f'{class_name}: {conf:.2f}'
            cv2.putText(frame, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        return frame


# Usage
detector = ObjectDetector(conf_threshold=0.6)
cap = cv2.VideoCapture(0)  # Webcam

while True:
    ret, frame = cap.read()
    if not ret:
        break

    detections = detector.detect(frame)
    frame = detector.draw_boxes(frame, detections)

    cv2.imshow('Object Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**Quiz: Object Detection**

What is the key innovation that makes YOLO significantly faster than R-CNN-based detectors?

[( )] YOLO uses a smaller CNN backbone
[(X)] YOLO frames detection as a single regression problem instead of multiple region proposals
[( )] YOLO uses binary classifiers
[( )] YOLO only detects one object per image

---

## Module 4: Visual Object Tracking

### 4.1 The Tracking Problem

<!-- class="theory-concept" -->
**Tracking vs. Detection**

**Detection**: Identify all objects in each frame independently
**Tracking**: Maintain consistent identity of objects across frames

**Challenges**:
- Occlusions
- Scale changes
- Appearance changes (lighting, rotation)
- Fast motion
- Similar-looking objects

**Tracking Approaches**:

1. **Detection-based tracking**: Run detector, associate detections across frames
2. **Correlation-based tracking**: Match templates/features between frames
3. **Model-based tracking**: Predict motion, update with observations (Kalman filter)
4. **Deep learning tracking**: Siamese networks, correlation filters

### 4.2 Correlation-Based Trackers

<!-- class="theory-concept" -->
**OpenCV Tracking Algorithms**

**KCF (Kernelized Correlation Filter)**:
- Exploits circulant structure for efficiency
- Ridge regression in Fourier domain
- Robust to partial occlusion
- Fast (~100 FPS)

**CSRT (Channel and Spatial Reliability Tracker)**:
- Uses spatial reliability map
- More robust than KCF
- Slower but more accurate

```python
# Initialize tracker with bounding box
tracker = cv2.TrackerKCF_create()
# or: tracker = cv2.TrackerCSRT_create()

# Read first frame
ret, frame = cap.read()
bbox = cv2.selectROI('Select Object', frame, False)

# Initialize tracker
tracker.init(frame, bbox)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Update tracker
    success, bbox = tracker.update(frame)

    if success:
        x, y, w, h = [int(v) for v in bbox]
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    cv2.imshow('Tracking', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```

### 4.3 Multi-Object Tracking

<!-- class="theory-concept" -->
**Data Association Problem**

Given detections at time t and t+1, which detection corresponds to which tracked object?

**Hungarian Algorithm** (optimal assignment):
- Compute cost matrix: C[i,j] = cost of associating detection i with track j
- Find optimal assignment minimizing total cost

**SORT (Simple Online Real-time Tracker)**:
1. Detect objects in each frame
2. Predict new locations of existing tracks (Kalman filter)
3. Associate detections to tracks (Hungarian algorithm)
4. Update tracks with associated detections
5. Create new tracks for unmatched detections
6. Delete tracks that haven't been updated

```python
from deep_sort_realtime.deepsort_tracker import DeepSort

tracker = DeepSort(max_age=30, n_init=3)

while True:
    ret, frame = cap.read()

    # Get detections (YOLOv5)
    detections = detector.detect(frame)

    # Format for tracker: [[left, top, width, height, confidence, class]]
    det_formatted = []
    for class_name, conf, x1, y1, x2, y2 in detections:
        w, h = x2 - x1, y2 - y1
        det_formatted.append([[x1, y1, w, h], conf, class_name])

    # Update tracker
    tracks = tracker.update_tracks(det_formatted, frame=frame)

    # Draw tracked objects
    for track in tracks:
        if not track.is_confirmed():
            continue
        track_id = track.track_id
        ltrb = track.to_ltrb()  # left, top, right, bottom

        x1, y1, x2, y2 = [int(x) for x in ltrb]
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f'ID: {track_id}', (x1, y1-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
```

### 4.4 From 2D Detection to 3D Localization

<!-- class="theory-concept" -->
**Estimating 3D Position**

Given a 2D bounding box, estimate 3D position (x, y, z) relative to camera.

**Assumptions**:
- Known camera intrinsics (focal length, principal point)
- Known object size or ground plane

**Pinhole Camera Model**:
$$u = f_x \frac{X}{Z} + c_x, \quad v = f_y \frac{Y}{Z} + c_y$$

where (u, v) are pixel coordinates, (X, Y, Z) are 3D coordinates, and (fx, fy, cx, cy) are camera parameters.

**Ground Plane Method**:
If object sits on known ground plane (Z = 0):
1. Detect bottom-center of bounding box: (u, v)
2. Invert camera projection to get (X, Y) on ground plane

```python
def pixel_to_world(u, v, camera_matrix, camera_height):
    """
    Convert pixel coordinates to world coordinates on ground plane.

    Args:
        u, v: Pixel coordinates
        camera_matrix: 3x3 camera intrinsic matrix
        camera_height: Height of camera above ground (meters)

    Returns:
        (x, y): World coordinates on ground plane (meters)
    """
    fx = camera_matrix[0, 0]
    fy = camera_matrix[1, 1]
    cx = camera_matrix[0, 2]
    cy = camera_matrix[1, 2]

    # Normalize pixel coordinates
    x_norm = (u - cx) / fx
    y_norm = (v - cy) / fy

    # Project to ground plane
    z = camera_height
    scale = z / y_norm
    x = x_norm * scale
    y = scale

    return x, y
```

**Quiz: Tracking**

Why is multi-object tracking more challenging than single-object tracking?

[( )] Multi-object tracking requires more computational power
[(X)] Multi-object tracking requires solving the data association problem
[( )] Multi-object tracking needs higher resolution cameras
[( )] Multi-object tracking only works indoors

---

## Module 5: Integration with Robot Systems

### 5.1 ROS2 Vision Node

<!-- class="theory-concept" -->
**Perception as a ROS2 Node**

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D, BoundingBox2D
from cv_bridge import CvBridge
import cv2
import torch

class VisionNode(Node):
    """ROS2 node for real-time object detection."""

    def __init__(self):
        super().__init__('vision_node')

        # Publishers and subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10)
        self.detection_pub = self.create_publisher(
            Detection2DArray, '/vision/detections', 10)

        # OpenCV bridge
        self.bridge = CvBridge()

        # Load detector
        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
        self.model.conf = 0.5

        self.get_logger().info('Vision node initialized')

    def image_callback(self, msg):
        """Process incoming image and publish detections."""
        # Convert ROS image to OpenCV
        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Run detection
        results = self.model(frame)

        # Create detection message
        detection_array = Detection2DArray()
        detection_array.header = msg.header

        for *box, conf, cls in results.xyxy[0]:
            detection = Detection2D()

            # Bounding box
            x1, y1, x2, y2 = [int(x) for x in box]
            bbox = BoundingBox2D()
            bbox.center.position.x = float((x1 + x2) / 2)
            bbox.center.position.y = float((y1 + y2) / 2)
            bbox.size_x = float(x2 - x1)
            bbox.size_y = float(y2 - y1)
            detection.bbox = bbox

            # Class and confidence
            detection.id = self.model.names[int(cls)]
            detection.score = float(conf)

            detection_array.detections.append(detection)

        # Publish
        self.detection_pub.publish(detection_array)

def main(args=None):
    rclpy.init(args=args)
    node = VisionNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
```

### 5.2 Performance Optimization

<!-- class="theory-concept" -->
**Real-Time Considerations**

**Model Selection Trade-offs**:

| Model | FPS (GPU) | mAP | Size | Use Case |
|-------|-----------|-----|------|----------|
| YOLOv5n | 455 | 28.0 | 4 MB | Embedded systems |
| YOLOv5s | 187 | 37.4 | 14 MB | Mobile devices |
| YOLOv5m | 108 | 45.4 | 42 MB | Desktop applications |
| YOLOv5l | 76 | 49.0 | 92 MB | High accuracy |
| YOLOv5x | 53 | 50.7 | 166 MB | Max accuracy |

**Optimization Techniques**:

1. **Model Quantization**:
   - FP32 → FP16 (2× speedup, minimal accuracy loss)
   - FP32 → INT8 (4× speedup, small accuracy loss)

2. **TensorRT Optimization** (NVIDIA GPUs):
   - Fuses layers
   - Optimizes kernel selection
   - 2-5× speedup

3. **ONNX Runtime**:
   - Cross-platform inference
   - Optimized for edge devices

```python
# Export to ONNX
model.export(format='onnx')

# Load ONNX model
import onnxruntime as ort
session = ort.InferenceSession('yolov5s.onnx')
```

### 5.3 Failure Modes and Robustness

<!-- class="theory-concept" -->
**Common Vision System Failures**

1. **Lighting Changes**: Cast shadows, reflections, extreme brightness/darkness
   - Mitigation: Auto-exposure, histogram equalization, train on diverse lighting

2. **Occlusions**: Object partially hidden
   - Mitigation: Track through occlusions, multi-view systems

3. **Motion Blur**: Fast robot or object motion
   - Mitigation: Higher frame rate, motion deblurring, prediction

4. **Out-of-Distribution**: Objects/scenes not in training data
   - Mitigation: Uncertainty estimation, anomaly detection, human-in-loop

5. **False Positives/Negatives**: Incorrect detections
   - Mitigation: Temporal filtering, confidence thresholds, sensor fusion

**Temporal Filtering Example**:

```python
class TemporalFilter:
    """Filter detections over time to reduce false positives."""

    def __init__(self, window_size=5, threshold=0.6):
        self.window_size = window_size
        self.threshold = threshold
        self.history = []

    def update(self, detections):
        """
        Add new detections and return filtered detections.

        Args:
            detections: List of (class, confidence, x1, y1, x2, y2)

        Returns:
            filtered: Detections seen in >= threshold of recent frames
        """
        self.history.append(detections)
        if len(self.history) > self.window_size:
            self.history.pop(0)

        # Count occurrences of each class
        class_counts = {}
        for frame_dets in self.history:
            for class_name, _, _, _, _, _ in frame_dets:
                class_counts[class_name] = class_counts.get(class_name, 0) + 1

        # Filter: keep classes seen in >= threshold of frames
        threshold_count = self.threshold * len(self.history)
        filtered = []
        for det in detections:
            if class_counts.get(det[0], 0) >= threshold_count:
                filtered.append(det)

        return filtered
```

---

## Summary

This course established comprehensive understanding of computer vision for robotics:

1. **Image Processing**: Fundamental operations for image manipulation and enhancement
2. **Classical Features**: Corner detection, SIFT/ORB for robust feature matching
3. **Deep Learning Detection**: YOLO and modern object detectors for real-time perception
4. **Visual Tracking**: Single and multi-object tracking for temporal consistency
5. **Robot Integration**: ROS2 vision nodes and practical deployment considerations

**Key Takeaways**:
- Vision systems transform raw pixels into structured understanding of the environment
- Modern deep learning detectors achieve real-time performance suitable for robotics
- Tracking adds temporal consistency and enables prediction
- Practical systems require careful consideration of failure modes and optimization

**Next Steps**: AI-203 explores imitation learning, where visual perception enables robots to learn complex behaviors from demonstration.

---

## Optional Laboratory Exercises

### Lab 1: Color-Based Object Detection (1.5 hours)

<!-- class="optional-exercise" -->
**Objective**: Build a classical computer vision pipeline to detect colored objects.

**Tasks**:
1. Implement HSV color thresholding for red, green, blue objects
2. Use morphological operations to clean up mask
3. Find contours and draw bounding boxes
4. Filter detections by area to remove noise
5. Display real-time results from webcam

<!-- class="exercise-tip" -->
**Tips**:
- Use `cv2.inRange()` for color thresholding
- Red color wraps around in HSV (0-10° and 350-360°)
- Chain morphological opening and closing
- Test with different lighting conditions

### Lab 2: Real-Time Object Detection with YOLO (2 hours)

<!-- class="optional-exercise" -->
**Objective**: Deploy YOLOv5 for real-time object detection from webcam feed.

**Tasks**:
1. Load pre-trained YOLOv5 model
2. Implement video capture loop
3. Run inference and visualize detections
4. Measure and display FPS
5. Filter detections by class (e.g., only "person" and "cup")
6. Log detections to file with timestamps

**Target Performance**: >15 FPS on GPU, >5 FPS on CPU

<!-- class="exercise-advanced" -->
**Advanced Extensions**:
- Implement ROI (region of interest) filtering
- Add audio alerts when specific objects detected
- Export detections to JSON for offline analysis
- Compare YOLOv5s vs YOLOv5m performance

### Lab 3: Multi-Object Tracking System (2.5 hours)

<!-- class="optional-exercise" -->
**Objective**: Build complete detection + tracking pipeline with 3D localization.

**Tasks**:
1. Integrate YOLOv5 detector with DeepSORT tracker
2. Maintain consistent object IDs across frames
3. Implement temporal filtering to reduce false positives
4. Estimate 3D positions on ground plane (given camera calibration)
5. Visualize tracks with unique colors per ID
6. Display object trajectories over time

<!-- class="exercise-tip" -->
**Implementation Notes**:
- Use DeepSORT for robust multi-object tracking
- Draw trajectory as line connecting past positions
- Store last N positions per track for trajectory visualization
- Test with multiple similar objects (e.g., multiple cups)

**Deliverable**: Video showing stable tracking of 3+ objects with consistent IDs and position estimates.

---

*Version 2.0.0 - Academic curriculum emphasizing theoretical foundations with optional practical exercises*
